{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# BeautifulSoup is used to remove html tags from the text\n",
    "from bs4 import BeautifulSoup \n",
    "import re # For regular expressions\n",
    "\n",
    "# Packages for data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# Packages for machine learning modelling\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "# precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Packages for sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Packages for visualisation \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Packages for MLP\n",
    "import nltk\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "train_data = pd.read_csv(\"../Data/train_data.csv\")\n",
    "val_data = pd.read_csv(\"../Data/validation_data.csv\")\n",
    "test_data = pd.read_csv(\"../Data/test_data.csv\")\n",
    "\n",
    "X_train_text = train_data[\"text_preprocessed\"].values\n",
    "y_train = train_data[\"class_label\"].values\n",
    "\n",
    "X_val_text = val_data[\"text_preprocessed\"].values\n",
    "y_val = val_data[\"class_label\"].values\n",
    "\n",
    "X_test_text = test_data[\"text_preprocessed\"].values\n",
    "y_test = test_data[\"class_label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of lists of Unigrams based on training corpus\n",
    "lst_corpus = []\n",
    "for string in X_train_text:\n",
    "    lst_words = string.split()\n",
    "    lst_grams = [\" \".join(lst_words[i:i+1]) \n",
    "                for i in range(0, len(lst_words), 1)]\n",
    "    lst_corpus.append(lst_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean length = 311.68333333333334\n"
     ]
    }
   ],
   "source": [
    "# Find mean length\n",
    "num_articles = len(lst_corpus)\n",
    "total_len = 0\n",
    "for article in lst_corpus:\n",
    "    total_len += len(article)\n",
    "\n",
    "print('mean length =', total_len/num_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    }
   ],
   "source": [
    "# Creating the model and setting values for the various parameters\n",
    "corpus = lst_corpus # Must be a list of lists of tokens\n",
    "num_features = 300  # Word vector dimensionality, i.e. target size of word vectors\n",
    "min_word_count = 10 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 311        # Context window size, possible to use mean length of text in corpus\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "skipgram = 1 # Use skipgram = 1\n",
    "\n",
    "# Initializing the train model\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences = corpus,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context,\n",
    "    sample=downsampling,\n",
    "    sg=skipgram\n",
    ")\n",
    "\n",
    "# To make the model memory efficient\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"300features_10minwords_311context_skip\"\n",
    "model.save(model_name)\n",
    "print(\"model saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model.\n",
    "# model = gensim.models.Word2Vec.load('300features_40minwords_10context')\n",
    "# model = gensim.models.Word2Vec.load('300features_10minwords_311context')\n",
    "model = gensim.models.Word2Vec.load('Pretrained_Word2Vec/300features_10minwords_311context_skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vocab: 29926\n"
     ]
    }
   ],
   "source": [
    "words = list(model.wv.vocab)\n",
    "print('length of vocab:', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donald', 0.9013704061508179),\n",
       " ('republican', 0.7219638824462891),\n",
       " ('presid', 0.7009391784667969),\n",
       " ('presidenti', 0.6926494836807251),\n",
       " ('campaign', 0.6864993572235107),\n",
       " ('support', 0.6612122058868408),\n",
       " ('candid', 0.6407409310340881),\n",
       " ('nomine', 0.601711630821228),\n",
       " ('democrat', 0.5973963141441345),\n",
       " ('obama', 0.5808975696563721)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of similar words\n",
    "w1=\"trump\"\n",
    "model.wv.most_similar(positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word_vec dictionary\n",
    "word_vec = dict({})\n",
    "for idx, key, in enumerate(model.wv.vocab):\n",
    "    word_vec[key] = model.wv[key]\n",
    "# print(\"input word vectors:\")\n",
    "# print(word_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39060\n",
      "39060\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "X_train = []\n",
    "text = X_train_text\n",
    "for i in range(len(text)):\n",
    "    doc_vec_for_each_sent = np.array(300)\n",
    "    for j in range(len(text[i])):\n",
    "        word = text[i][j]\n",
    "        if word in word_vec.keys():\n",
    "            doc_vec_for_each_sent = np.add(doc_vec_for_each_sent,\n",
    "                                            word_vec[word])\n",
    "    doc_vec_for_each_sent = np.divide(doc_vec_for_each_sent, len(text[i]))\n",
    "    X_train.append(doc_vec_for_each_sent)\n",
    "# print('Generated Document Vectors:')\n",
    "# print(X_train)\n",
    "print(len(X_train))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13020\n",
      "13020\n"
     ]
    }
   ],
   "source": [
    "# Val\n",
    "X_val = []\n",
    "text = X_val_text\n",
    "for i in range(len(text)):\n",
    "    doc_vec_for_each_sent = np.array(300)\n",
    "    for j in range(len(text[i])):\n",
    "        word = text[i][j]\n",
    "        if word in word_vec.keys():\n",
    "            doc_vec_for_each_sent = np.add(doc_vec_for_each_sent,\n",
    "                                            word_vec[word])\n",
    "    doc_vec_for_each_sent = np.divide(doc_vec_for_each_sent, len(text[i]))\n",
    "    X_val.append(doc_vec_for_each_sent)\n",
    "# print('Generated Document Vectors:')\n",
    "# print(X_val)\n",
    "print(len(X_val))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13021\n",
      "13021\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "X_test = []\n",
    "text = X_test_text\n",
    "for i in range(len(text)):\n",
    "    doc_vec_for_each_sent = np.array(300)\n",
    "    for j in range(len(text[i])):\n",
    "        word = text[i][j]\n",
    "        if word in word_vec.keys():\n",
    "            doc_vec_for_each_sent = np.add(doc_vec_for_each_sent,\n",
    "                                            word_vec[word])\n",
    "    doc_vec_for_each_sent = np.divide(doc_vec_for_each_sent, len(text[i]))\n",
    "    X_test.append(doc_vec_for_each_sent)\n",
    "# print('Generated Document Vectors:')\n",
    "# print(X_test)\n",
    "print(len(X_test))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.66      6361\n",
      "           1       0.68      0.68      0.68      6659\n",
      "\n",
      "    accuracy                           0.67     13020\n",
      "   macro avg       0.67      0.67      0.67     13020\n",
      "weighted avg       0.67      0.67      0.67     13020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logreg\n",
    "log_reg_clf = LogisticRegression()\n",
    "log_reg_clf.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = log_reg_clf.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.97      0.65      6361\n",
      "           1       0.60      0.04      0.08      6659\n",
      "\n",
      "    accuracy                           0.50     13020\n",
      "   macro avg       0.54      0.51      0.37     13020\n",
      "weighted avg       0.55      0.50      0.36     13020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NB\n",
    "naive_bayes_clf = BernoulliNB()\n",
    "naive_bayes_clf.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = naive_bayes_clf.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
