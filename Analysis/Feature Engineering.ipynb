{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# Packages for NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "\n",
    "# Packages for sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Packages for visualisation \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../Data/train_data.csv\", index_col=1)\n",
    "val_data = pd.read_csv(\"../Data/validation_data.csv\", index_col=1)\n",
    "test_data = pd.read_csv(\"../Data/test_data.csv\", index_col=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "discourse_keywords = 'although, as though, but, by comparison, even if, even though, however, nevertheless, on the other hand, still, then, though, while, yet, and, meanwhile, in turn, next, ultimately, meantime, also, as if, even as, even still, even then, regardless, when, by contrast, conversely, if, in contrast, instead, nor, or, rather, whereas, while, yet, even after, by contrast, nevertheless, besides, much as, as much as, whereas, neither, nonetheless, even when, on the one hand indeed, finally, in fact, separately, in the end, on the contrary, while'\n",
    "discourse_keywords = discourse_keywords.split(', ')\n",
    "discourse_keywords = list(set(discourse_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags reference: https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/ \n",
    "nouns_list = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "verbs_list = ['VB', 'VBD', 'VBG', 'VBN', 'VDP', 'VBZ']\n",
    "adj_list = ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "def get_pos_tags(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    return pos_tags\n",
    "\n",
    "def get_num_nouns(text):\n",
    "    pos_tags = get_pos_tags(text)\n",
    "    nouns_count = len([word for (word, pos) in pos_tags if pos in nouns_list])\n",
    "    return nouns_count\n",
    "\n",
    "def get_num_verbs(text):\n",
    "    pos_tags = get_pos_tags(text)\n",
    "    verbs_count = len([word for (word, pos) in pos_tags if pos in verbs_list])\n",
    "    return verbs_count\n",
    "\n",
    "def get_num_adj(text):\n",
    "    pos_tags = get_pos_tags(text)\n",
    "    adj_count = len([word for (word, pos) in pos_tags if pos in adj_list])\n",
    "    return adj_count\n",
    "\n",
    "def get_num_discourse(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    discourse_count = len([word for word in tokens if word in discourse_keywords])\n",
    "    return discourse_count\n",
    "\n",
    "def get_num_stopwords(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stopword_count = len([word for word in tokens if word in stopwords.words('english')])\n",
    "    return stopword_count\n",
    "\n",
    "def get_num_punctuations(text):\n",
    "    punctuations = '!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~'\n",
    "    punctuation_count = len([char for char in text if char in punctuations])\n",
    "    return punctuation_count\n",
    "\n",
    "def get_num_words_in_quotes(text):\n",
    "    quotes = re.findall(\"'.'|\\\".\\\"\", text)\n",
    "    quote_count = 0\n",
    "    if quotes is None:\n",
    "        return 0\n",
    "    else:\n",
    "        for quote in quotes:\n",
    "            words_in_quote = quote[1:-1]\n",
    "            quote_count += len(words_in_quote.split())\n",
    "        return quote_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding features to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_nouns = []\n",
    "train_num_verbs = []\n",
    "train_num_adj = []\n",
    "train_num_discourse = []\n",
    "train_num_stopwords = []\n",
    "train_num_punctuations = []\n",
    "train_num_quote_words = []\n",
    "\n",
    "for row in train_data['text']:\n",
    "    nouns_count = get_num_nouns(row)\n",
    "    train_num_nouns.append(nouns_count)\n",
    "\n",
    "    verbs_count = get_num_verbs(row)\n",
    "    train_num_verbs.append(verbs_count)\n",
    "    \n",
    "    adj_count = get_num_adj(row)\n",
    "    train_num_adj.append(adj_count)\n",
    "    \n",
    "    discourse_count = get_num_discourse(row)\n",
    "    train_num_discourse.append(discourse_count)\n",
    "\n",
    "    stopword_count = get_num_stopwords(row)\n",
    "    train_num_stopwords.append(stopword_count)\n",
    "\n",
    "    punctuation_count = get_num_punctuations(row)\n",
    "    train_num_punctuations.append(punctuation_count)\n",
    "\n",
    "    quote_count = get_num_words_in_quotes(row)\n",
    "    train_num_quote_words.append(quote_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['char_count'] = train_data['text'].apply(lambda x: len(str(x)))\n",
    "train_data['word_count'] = train_data['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "train_data['sentence_count'] = train_data['text'].apply(lambda x: len(str(x).split(\".\")))\n",
    "train_data[\"num_unique_words\"] = train_data['text'].apply(lambda x: len(set(str(x).split(\" \"))))\n",
    "train_data[\"avg_sentence_length\"] = train_data['word_count']/train_data['sentence_count']\n",
    "train_data['num_punctuations'] = train_num_punctuations\n",
    "train_data['num_stopwords'] = train_num_stopwords\n",
    "train_data['num_words_in_quotes'] = train_num_quote_words\n",
    "\n",
    "train_data['num_nouns'] = train_num_nouns\n",
    "train_data['num_verbs'] = train_num_verbs\n",
    "train_data['num_adjectives'] = train_num_adj\n",
    "train_data['num_discourse_relations'] = train_num_discourse\n",
    "\n",
    "train_data['textblob_sentiment'] = train_data['text'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"train_data_with_added_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding features to validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>class_label</th>\n",
       "      <th>text_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Britain's May presses Northern Ireland leaders...</td>\n",
       "      <td>20227</td>\n",
       "      <td>0</td>\n",
       "      <td>britain may press northern ireland leader rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GOP EVIDENCE: Comey FBI Busted Giving Clinton ...</td>\n",
       "      <td>9356</td>\n",
       "      <td>1</td>\n",
       "      <td>gop evid comey fbi bust give clinton special s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Want to See ‘Hamilton’ in a City Near You? Buy...</td>\n",
       "      <td>1437</td>\n",
       "      <td>0</td>\n",
       "      <td>want see hamilton citi near buy subscript wait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why 'Never Trumpers' must reconsiderWhile Demo...</td>\n",
       "      <td>8457</td>\n",
       "      <td>1</td>\n",
       "      <td>never trumper must reconsiderwhil democrat per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump Says Health Law Replacement May Not Be R...</td>\n",
       "      <td>11482</td>\n",
       "      <td>0</td>\n",
       "      <td>trump say health law replac may readi next yea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Unnamed: 0  class_label  \\\n",
       "0  Britain's May presses Northern Ireland leaders...       20227            0   \n",
       "1  GOP EVIDENCE: Comey FBI Busted Giving Clinton ...        9356            1   \n",
       "2  Want to See ‘Hamilton’ in a City Near You? Buy...        1437            0   \n",
       "3  Why 'Never Trumpers' must reconsiderWhile Demo...        8457            1   \n",
       "4  Trump Says Health Law Replacement May Not Be R...       11482            0   \n",
       "\n",
       "                                   text_preprocessed  \n",
       "0  britain may press northern ireland leader rest...  \n",
       "1  gop evid comey fbi bust give clinton special s...  \n",
       "2  want see hamilton citi near buy subscript wait...  \n",
       "3  never trumper must reconsiderwhil democrat per...  \n",
       "4  trump say health law replac may readi next yea...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.reset_index(inplace=True)\n",
    "val_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_num_nouns = []\n",
    "val_num_verbs = []\n",
    "val_num_adj = []\n",
    "val_num_discourse = []\n",
    "val_num_stopwords = []\n",
    "val_num_punctuations = []\n",
    "val_num_quote_words = []\n",
    "\n",
    "for row in val_data['text']:\n",
    "    nouns_count = get_num_nouns(row)\n",
    "    val_num_nouns.append(nouns_count)\n",
    "\n",
    "    verbs_count = get_num_verbs(row)\n",
    "    val_num_verbs.append(verbs_count)\n",
    "    \n",
    "    adj_count = get_num_adj(row)\n",
    "    val_num_adj.append(adj_count)\n",
    "    \n",
    "    discourse_count = get_num_discourse(row)\n",
    "    val_num_discourse.append(discourse_count)\n",
    "\n",
    "    stopword_count = get_num_stopwords(row)\n",
    "    val_num_stopwords.append(stopword_count)\n",
    "\n",
    "    punctuation_count = get_num_punctuations(row)\n",
    "    val_num_punctuations.append(punctuation_count)\n",
    "\n",
    "    quote_count = get_num_words_in_quotes(row)\n",
    "    val_num_quote_words.append(quote_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data['char_count'] = val_data['text'].apply(lambda x: len(str(x)))\n",
    "val_data['word_count'] = val_data['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "val_data['sentence_count'] = val_data['text'].apply(lambda x: len(str(x).split(\".\")))\n",
    "val_data[\"num_unique_words\"] = val_data['text'].apply(lambda x: len(set(str(x).split(\" \"))))\n",
    "val_data[\"avg_sentence_length\"] = val_data['word_count']/val_data['sentence_count']\n",
    "val_data['num_punctuations'] = val_num_punctuations\n",
    "val_data['num_stopwords'] = val_num_stopwords\n",
    "val_data['num_words_in_quotes'] = val_num_quote_words\n",
    "\n",
    "val_data['num_nouns'] = val_num_nouns\n",
    "val_data['num_verbs'] = val_num_verbs\n",
    "val_data['num_adjectives'] = val_num_adj\n",
    "val_data['num_discourse_relations'] = val_num_discourse\n",
    "\n",
    "val_data['textblob_sentiment'] = val_data['text'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.to_csv(\"val_data_with_added_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>class_label</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_words_in_quotes</th>\n",
       "      <th>num_nouns</th>\n",
       "      <th>num_verbs</th>\n",
       "      <th>num_adjectives</th>\n",
       "      <th>num_discourse_relations</th>\n",
       "      <th>textblob_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Britain's May presses Northern Ireland leaders...</td>\n",
       "      <td>20227</td>\n",
       "      <td>0</td>\n",
       "      <td>britain may press northern ireland leader rest...</td>\n",
       "      <td>986</td>\n",
       "      <td>159</td>\n",
       "      <td>5</td>\n",
       "      <td>102</td>\n",
       "      <td>31.800000</td>\n",
       "      <td>16</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.045833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GOP EVIDENCE: Comey FBI Busted Giving Clinton ...</td>\n",
       "      <td>9356</td>\n",
       "      <td>1</td>\n",
       "      <td>gop evid comey fbi bust give clinton special s...</td>\n",
       "      <td>3411</td>\n",
       "      <td>583</td>\n",
       "      <td>24</td>\n",
       "      <td>285</td>\n",
       "      <td>24.291667</td>\n",
       "      <td>60</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>223</td>\n",
       "      <td>88</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>0.127709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Want to See ‘Hamilton’ in a City Near You? Buy...</td>\n",
       "      <td>1437</td>\n",
       "      <td>0</td>\n",
       "      <td>want see hamilton citi near buy subscript wait...</td>\n",
       "      <td>5837</td>\n",
       "      <td>1019</td>\n",
       "      <td>49</td>\n",
       "      <td>499</td>\n",
       "      <td>20.795918</td>\n",
       "      <td>126</td>\n",
       "      <td>427</td>\n",
       "      <td>0</td>\n",
       "      <td>377</td>\n",
       "      <td>173</td>\n",
       "      <td>77</td>\n",
       "      <td>40</td>\n",
       "      <td>0.183048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why 'Never Trumpers' must reconsiderWhile Demo...</td>\n",
       "      <td>8457</td>\n",
       "      <td>1</td>\n",
       "      <td>never trumper must reconsiderwhil democrat per...</td>\n",
       "      <td>5662</td>\n",
       "      <td>931</td>\n",
       "      <td>61</td>\n",
       "      <td>539</td>\n",
       "      <td>15.262295</td>\n",
       "      <td>135</td>\n",
       "      <td>386</td>\n",
       "      <td>0</td>\n",
       "      <td>319</td>\n",
       "      <td>121</td>\n",
       "      <td>85</td>\n",
       "      <td>54</td>\n",
       "      <td>0.007988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump Says Health Law Replacement May Not Be R...</td>\n",
       "      <td>11482</td>\n",
       "      <td>0</td>\n",
       "      <td>trump say health law replac may readi next yea...</td>\n",
       "      <td>2609</td>\n",
       "      <td>453</td>\n",
       "      <td>25</td>\n",
       "      <td>264</td>\n",
       "      <td>18.120000</td>\n",
       "      <td>49</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>76</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>0.114201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Unnamed: 0  class_label  \\\n",
       "0  Britain's May presses Northern Ireland leaders...       20227            0   \n",
       "1  GOP EVIDENCE: Comey FBI Busted Giving Clinton ...        9356            1   \n",
       "2  Want to See ‘Hamilton’ in a City Near You? Buy...        1437            0   \n",
       "3  Why 'Never Trumpers' must reconsiderWhile Demo...        8457            1   \n",
       "4  Trump Says Health Law Replacement May Not Be R...       11482            0   \n",
       "\n",
       "                                   text_preprocessed  char_count  word_count  \\\n",
       "0  britain may press northern ireland leader rest...         986         159   \n",
       "1  gop evid comey fbi bust give clinton special s...        3411         583   \n",
       "2  want see hamilton citi near buy subscript wait...        5837        1019   \n",
       "3  never trumper must reconsiderwhil democrat per...        5662         931   \n",
       "4  trump say health law replac may readi next yea...        2609         453   \n",
       "\n",
       "   sentence_count  num_unique_words  avg_sentence_length  num_punctuations  \\\n",
       "0               5               102            31.800000                16   \n",
       "1              24               285            24.291667                60   \n",
       "2              49               499            20.795918               126   \n",
       "3              61               539            15.262295               135   \n",
       "4              25               264            18.120000                49   \n",
       "\n",
       "   num_stopwords  num_words_in_quotes  num_nouns  num_verbs  num_adjectives  \\\n",
       "0             53                    0         61         22              15   \n",
       "1            224                    0        223         88              40   \n",
       "2            427                    0        377        173              77   \n",
       "3            386                    0        319        121              85   \n",
       "4            171                    0        162         76              29   \n",
       "\n",
       "   num_discourse_relations  textblob_sentiment  \n",
       "0                        3            0.045833  \n",
       "1                       12            0.127709  \n",
       "2                       40            0.183048  \n",
       "3                       54            0.007988  \n",
       "4                       14            0.114201  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding features to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>class_label</th>\n",
       "      <th>text_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Billionaire Art Dealer Guy Wildenstein Is ...</td>\n",
       "      <td>12818</td>\n",
       "      <td>0</td>\n",
       "      <td>billionair art dealer guy wildenstein clear ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>POLICE UNION Threatens 49er’s With BOYCOTT: TA...</td>\n",
       "      <td>13097</td>\n",
       "      <td>1</td>\n",
       "      <td>polic union threaten 49er boycott take action ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>U.S.G.A. Regrets ‘Distraction’ in Ruling Again...</td>\n",
       "      <td>1727</td>\n",
       "      <td>0</td>\n",
       "      <td>usga regret distract rule dustin johnson new y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>FAKE NEWS WEEK: Truth, War Propaganda, CIA and...</td>\n",
       "      <td>22995</td>\n",
       "      <td>1</td>\n",
       "      <td>fake news week truth war propaganda cia media ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>WATCH: Reince Priebus Subtly But Hilariously ...</td>\n",
       "      <td>2404</td>\n",
       "      <td>1</td>\n",
       "      <td>watch reinc priebu subtli hilari embarrass ste...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index                                               text  \\\n",
       "0        0      0  The Billionaire Art Dealer Guy Wildenstein Is ...   \n",
       "1        1      1  POLICE UNION Threatens 49er’s With BOYCOTT: TA...   \n",
       "2        2      2  U.S.G.A. Regrets ‘Distraction’ in Ruling Again...   \n",
       "3        3      3  FAKE NEWS WEEK: Truth, War Propaganda, CIA and...   \n",
       "4        4      4   WATCH: Reince Priebus Subtly But Hilariously ...   \n",
       "\n",
       "   Unnamed: 0  class_label                                  text_preprocessed  \n",
       "0       12818            0  billionair art dealer guy wildenstein clear ta...  \n",
       "1       13097            1  polic union threaten 49er boycott take action ...  \n",
       "2        1727            0  usga regret distract rule dustin johnson new y...  \n",
       "3       22995            1  fake news week truth war propaganda cia media ...  \n",
       "4        2404            1  watch reinc priebu subtli hilari embarrass ste...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.reset_index(inplace=True)\n",
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num_nouns = []\n",
    "test_num_verbs = []\n",
    "test_num_adj = []\n",
    "test_num_discourse = []\n",
    "test_num_stopwords = []\n",
    "test_num_punctuations = []\n",
    "test_num_quote_words = []\n",
    "\n",
    "for row in test_data['text']:\n",
    "    nouns_count = get_num_nouns(row)\n",
    "    test_num_nouns.append(nouns_count)\n",
    "\n",
    "    verbs_count = get_num_verbs(row)\n",
    "    test_num_verbs.append(verbs_count)\n",
    "    \n",
    "    adj_count = get_num_adj(row)\n",
    "    test_num_adj.append(adj_count)\n",
    "    \n",
    "    discourse_count = get_num_discourse(row)\n",
    "    test_num_discourse.append(discourse_count)\n",
    "\n",
    "    stopword_count = get_num_stopwords(row)\n",
    "    test_num_stopwords.append(stopword_count)\n",
    "\n",
    "    punctuation_count = get_num_punctuations(row)\n",
    "    test_num_punctuations.append(punctuation_count)\n",
    "\n",
    "    quote_count = get_num_words_in_quotes(row)\n",
    "    test_num_quote_words.append(quote_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['char_count'] = test_data['text'].apply(lambda x: len(str(x)))\n",
    "test_data['word_count'] = test_data['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "test_data['sentence_count'] = test_data['text'].apply(lambda x: len(str(x).split(\".\")))\n",
    "test_data[\"num_unique_words\"] = test_data['text'].apply(lambda x: len(set(str(x).split(\" \"))))\n",
    "test_data[\"avg_sentence_length\"] = test_data['word_count']/test_data['sentence_count']\n",
    "test_data['num_punctuations'] = test_num_punctuations\n",
    "test_data['num_stopwords'] = test_num_stopwords\n",
    "test_data['num_words_in_quotes'] = test_num_quote_words\n",
    "\n",
    "test_data['num_nouns'] = test_num_nouns\n",
    "test_data['num_verbs'] = test_num_verbs\n",
    "test_data['num_adjectives'] = test_num_adj\n",
    "test_data['num_discourse_relations'] = test_num_discourse\n",
    "\n",
    "test_data['textblob_sentiment'] = test_data['text'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(\"test_data_with_added_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1,2), min_df=0.15)\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "    (['char_count', 'word_count', 'sentence_count', 'num_unique_words', 'avg_sentence_length', 'num_punctuations', 'num_stopwords', 'num_words_in_quotes', 'num_nouns', 'num_verbs', 'num_adjectives', 'num_discourse_relations', 'textblob_sentiment'], None), \n",
    "    ('text_preprocessed',tfidf_vectorizer)\n",
    "])\n",
    "\n",
    "X_train = mapper.fit_transform(train_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
