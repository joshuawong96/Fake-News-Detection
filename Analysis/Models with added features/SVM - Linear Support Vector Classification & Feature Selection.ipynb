{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Packages for data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# Packages for machine learning modelling\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "# precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Packages for sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "# Packages for visualisation \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Packages for NLP\n",
    "import nltk\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"new data/train_data.csv\", index_col=1)\n",
    "val_data = pd.read_csv(\"new data/validation_data.csv\", index_col=1)\n",
    "test_data = pd.read_csv(\"new data/test_data.csv\", index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = train_data[\"text_preprocessed\"].values\n",
    "y_train = train_data[\"class_label\"].values\n",
    "\n",
    "X_val_text = val_data[\"text_preprocessed\"].values\n",
    "y_val = val_data[\"class_label\"].values\n",
    "\n",
    "X_test_text = test_data[\"text_preprocessed\"].values\n",
    "y_test = test_data[\"class_label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model using SVM Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,1))\n",
    "vectorizer.fit(X_train_text)\n",
    "\n",
    "X_train = vectorizer.transform(X_train_text)\n",
    "X_val = vectorizer.transform(X_val_text)\n",
    "X_test = vectorizer.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features used: 238266\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features used:\", len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# Sparse vector of frequency of each word appearing in a text article\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf = LinearSVC()\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Model with unigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      6361\n",
      "           1       0.96      0.97      0.96      6659\n",
      "\n",
      "    accuracy                           0.96     13020\n",
      "   macro avg       0.96      0.96      0.96     13020\n",
      "weighted avg       0.96      0.96      0.96     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      6361\n",
      "           1       0.96      0.97      0.96      6660\n",
      "\n",
      "    accuracy                           0.96     13021\n",
      "   macro avg       0.96      0.96      0.96     13021\n",
      "weighted avg       0.96      0.96      0.96     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "CountVectorizer Model with unigram and bigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      6361\n",
      "           1       0.97      0.98      0.97      6659\n",
      "\n",
      "    accuracy                           0.97     13020\n",
      "   macro avg       0.97      0.97      0.97     13020\n",
      "weighted avg       0.97      0.97      0.97     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      6361\n",
      "           1       0.97      0.98      0.97      6660\n",
      "\n",
      "    accuracy                           0.97     13021\n",
      "   macro avg       0.97      0.97      0.97     13021\n",
      "weighted avg       0.97      0.97      0.97     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "CountVectorizer Model with bigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      6361\n",
      "           1       0.94      0.96      0.95      6659\n",
      "\n",
      "    accuracy                           0.95     13020\n",
      "   macro avg       0.95      0.95      0.95     13020\n",
      "weighted avg       0.95      0.95      0.95     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95      6361\n",
      "           1       0.94      0.97      0.95      6660\n",
      "\n",
      "    accuracy                           0.95     13021\n",
      "   macro avg       0.95      0.95      0.95     13021\n",
      "weighted avg       0.95      0.95      0.95     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer_params = {'unigram':(1,1), 'unigram and bigram': (1,2), 'bigram':(2,2)}\n",
    "\n",
    "for ngram, values in count_vectorizer_params.items():\n",
    "    vectorizer = CountVectorizer(stop_words='english', ngram_range=values)\n",
    "    vectorizer.fit(X_train_text)\n",
    "\n",
    "    X_train = vectorizer.transform(X_train_text)\n",
    "    X_val = vectorizer.transform(X_val_text)\n",
    "    X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "    print(f'CountVectorizer Model with {ngram}')\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "\n",
    "    #Validation Data\n",
    "    print('Testing with validation data:')\n",
    "    val_pred = svm_clf.predict(X_val)\n",
    "    print(classification_report(y_val, val_pred))\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "    # Test Data\n",
    "    print('Testing using test data:')\n",
    "    test_pred = svm_clf.predict(X_test)\n",
    "    print(classification_report(y_test, test_pred))\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with unigram\n",
      "Testing using validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      6361\n",
      "           1       0.97      0.98      0.97      6659\n",
      "\n",
      "    accuracy                           0.97     13020\n",
      "   macro avg       0.97      0.97      0.97     13020\n",
      "weighted avg       0.97      0.97      0.97     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      6361\n",
      "           1       0.97      0.97      0.97      6660\n",
      "\n",
      "    accuracy                           0.97     13021\n",
      "   macro avg       0.97      0.97      0.97     13021\n",
      "weighted avg       0.97      0.97      0.97     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "Model with unigram and bigram\n",
      "Testing using validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97      6361\n",
      "           1       0.98      0.97      0.98      6659\n",
      "\n",
      "    accuracy                           0.97     13020\n",
      "   macro avg       0.97      0.97      0.97     13020\n",
      "weighted avg       0.97      0.97      0.97     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97      6361\n",
      "           1       0.98      0.97      0.97      6660\n",
      "\n",
      "    accuracy                           0.97     13021\n",
      "   macro avg       0.97      0.97      0.97     13021\n",
      "weighted avg       0.97      0.97      0.97     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "Model with bigram\n",
      "Testing using validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      6361\n",
      "           1       0.97      0.97      0.97      6659\n",
      "\n",
      "    accuracy                           0.97     13020\n",
      "   macro avg       0.97      0.97      0.97     13020\n",
      "weighted avg       0.97      0.97      0.97     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      6361\n",
      "           1       0.97      0.97      0.97      6660\n",
      "\n",
      "    accuracy                           0.97     13021\n",
      "   macro avg       0.97      0.97      0.97     13021\n",
      "weighted avg       0.97      0.97      0.97     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tfidf_params = {'unigram':(1,1), 'unigram and bigram': (1,2), 'bigram':(2,2)}\n",
    "\n",
    "for ngram, values in tfidf_params.items():\n",
    "    tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=values)\n",
    "    tfidf_vectorizer.fit(X_train_text)\n",
    "\n",
    "    X_train = tfidf_vectorizer.transform(X_train_text)\n",
    "    X_val = tfidf_vectorizer.transform(X_val_text)\n",
    "    X_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "    svm_clf = LinearSVC()\n",
    "    print(f\"Model with {ngram}\")\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Validation Data\n",
    "    print(\"Testing using validation data:\")    \n",
    "    y_val_pred = svm_clf.predict(X_val)\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "    # Test Data\n",
    "    print(\"Testing using test data:\")\n",
    "    y_test_pred = svm_clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_df = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------CountVectorizer--------------------\n",
      "CountVectorizer Model with min_df=0.01\n",
      "3373\n",
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94      6361\n",
      "           1       0.94      0.95      0.95      6659\n",
      "\n",
      "    accuracy                           0.95     13020\n",
      "   macro avg       0.95      0.95      0.95     13020\n",
      "weighted avg       0.95      0.95      0.95     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95      6361\n",
      "           1       0.95      0.95      0.95      6660\n",
      "\n",
      "    accuracy                           0.95     13021\n",
      "   macro avg       0.95      0.95      0.95     13021\n",
      "weighted avg       0.95      0.95      0.95     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "print('--------------------CountVectorizer--------------------')\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.01)\n",
    "vectorizer.fit(X_train_text)\n",
    "\n",
    "X_train = vectorizer.transform(X_train_text)\n",
    "X_val = vectorizer.transform(X_val_text)\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f'CountVectorizer Model with min_df=0.01')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "num_features = len(vectorizer.get_feature_names())\n",
    "print(num_features)\n",
    "# countvectorizer_numfeatures.append(num_features)\n",
    "\n",
    "#Validation Data\n",
    "print('Testing with validation data:')\n",
    "val_pred = svm_clf.predict(X_val)\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Test Data\n",
    "print('Testing using test data:')\n",
    "test_pred = svm_clf.predict(X_test)\n",
    "report = classification_report(y_test, test_pred)\n",
    "print(report)\n",
    "print(\"------------------------------------------\")\n",
    "print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_df = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------CountVectorizer--------------------\n",
      "CountVectorizer Model with min_df=0.15\n",
      "134\n",
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.86      0.90      6361\n",
      "           1       0.88      0.95      0.91      6659\n",
      "\n",
      "    accuracy                           0.91     13020\n",
      "   macro avg       0.91      0.91      0.91     13020\n",
      "weighted avg       0.91      0.91      0.91     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.90      6361\n",
      "           1       0.88      0.95      0.92      6660\n",
      "\n",
      "    accuracy                           0.91     13021\n",
      "   macro avg       0.91      0.91      0.91     13021\n",
      "weighted avg       0.91      0.91      0.91     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "print('--------------------CountVectorizer--------------------')\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.15)\n",
    "vectorizer.fit(X_train_text)\n",
    "\n",
    "X_train = vectorizer.transform(X_train_text)\n",
    "X_val = vectorizer.transform(X_val_text)\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f'CountVectorizer Model with min_df=0.15')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "num_features = len(vectorizer.get_feature_names())\n",
    "print(num_features)\n",
    "\n",
    "#Validation Data\n",
    "print('Testing with validation data:')\n",
    "val_pred = svm_clf.predict(X_val)\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Test Data\n",
    "print('Testing using test data:')\n",
    "test_pred = svm_clf.predict(X_test)\n",
    "report = classification_report(y_test, test_pred)\n",
    "print(report)\n",
    "print(\"------------------------------------------\")\n",
    "print(\"------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_df = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------TF-IDF--------------------\n",
      "TF-IDF Model with min_df=0.01\n",
      "3373\n",
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      6361\n",
      "           1       0.96      0.97      0.96      6659\n",
      "\n",
      "    accuracy                           0.96     13020\n",
      "   macro avg       0.96      0.96      0.96     13020\n",
      "weighted avg       0.96      0.96      0.96     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      6361\n",
      "           1       0.96      0.96      0.96      6660\n",
      "\n",
      "    accuracy                           0.96     13021\n",
      "   macro avg       0.96      0.96      0.96     13021\n",
      "weighted avg       0.96      0.96      0.96     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------TF-IDF--------------------')\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.01)\n",
    "tfidf_vectorizer.fit(X_train_text)\n",
    "\n",
    "X_train = tfidf_vectorizer.transform(X_train_text)\n",
    "X_val = tfidf_vectorizer.transform(X_val_text)\n",
    "X_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f'TF-IDF Model with min_df=0.01')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "num_features = len(tfidf_vectorizer.get_feature_names())\n",
    "print(num_features)\n",
    "# tfidf_numfeatures.append(num_features)\n",
    "\n",
    "#Validation Data\n",
    "print('Testing with validation data:')\n",
    "val_pred = svm_clf.predict(X_val)\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Test Data\n",
    "print('Testing using test data:')\n",
    "test_pred = svm_clf.predict(X_test)\n",
    "report = classification_report(y_test, test_pred)\n",
    "print(report)\n",
    "print(\"------------------------------------------\")\n",
    "print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_df = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------TF-IDF--------------------\n",
      "TF-IDF Model with min_df=0.15\n",
      "134\n",
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      6361\n",
      "           1       0.90      0.92      0.91      6659\n",
      "\n",
      "    accuracy                           0.90     13020\n",
      "   macro avg       0.90      0.90      0.90     13020\n",
      "weighted avg       0.90      0.90      0.90     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      6361\n",
      "           1       0.90      0.92      0.91      6660\n",
      "\n",
      "    accuracy                           0.91     13021\n",
      "   macro avg       0.91      0.91      0.91     13021\n",
      "weighted avg       0.91      0.91      0.91     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------TF-IDF--------------------')\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.15)\n",
    "tfidf_vectorizer.fit(X_train_text)\n",
    "\n",
    "X_train = tfidf_vectorizer.transform(X_train_text)\n",
    "X_val = tfidf_vectorizer.transform(X_val_text)\n",
    "X_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f'TF-IDF Model with min_df=0.15')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "num_features = len(tfidf_vectorizer.get_feature_names())\n",
    "print(num_features)\n",
    "# tfidf_numfeatures.append(num_features)\n",
    "\n",
    "#Validation Data\n",
    "print('Testing with validation data:')\n",
    "val_pred = svm_clf.predict(X_val)\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Test Data\n",
    "print('Testing using test data:')\n",
    "test_pred = svm_clf.predict(X_test)\n",
    "report = classification_report(y_test, test_pred)\n",
    "print(report)\n",
    "print(\"------------------------------------------\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# precision, recall, f1score = get_weighted_average(report)\n",
    "# tfidf_precision.append(precision)\n",
    "# tfidf_recall.append(recall)\n",
    "# tfidf_f1score.append(f1score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_features instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Model with min_df=0.2\n",
      "10000\n",
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      6361\n",
      "           1       0.97      0.97      0.97      6659\n",
      "\n",
      "    accuracy                           0.97     13020\n",
      "   macro avg       0.97      0.97      0.97     13020\n",
      "weighted avg       0.97      0.97      0.97     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97      6361\n",
      "           1       0.97      0.97      0.97      6660\n",
      "\n",
      "    accuracy                           0.97     13021\n",
      "   macro avg       0.97      0.97      0.97     13021\n",
      "weighted avg       0.97      0.97      0.97     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_features=10000) #, max_df=max_value\n",
    "tfidf_vectorizer.fit(X_train_text)\n",
    "\n",
    "X_train = tfidf_vectorizer.transform(X_train_text)\n",
    "X_val = tfidf_vectorizer.transform(X_val_text)\n",
    "X_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f'TF-IDF Model with min_df=0.2') #, max_df={max_value}\n",
    "svm_clf.fit(X_train, y_train)\n",
    "print(len(tfidf_vectorizer.get_feature_names()))\n",
    "\n",
    "#Validation Data\n",
    "print('Testing with validation data:')\n",
    "val_pred = svm_clf.predict(X_val)\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Test Data\n",
    "print('Testing using test data:')\n",
    "test_pred = svm_clf.predict(X_test)\n",
    "print(classification_report(y_test, test_pred))\n",
    "print(\"------------------------------------------\")\n",
    "print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding out which words were eliminated and kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '10', '10 percent', '10 year', '100', '1000', '10000', '100000', '11', '12', '13', '14', '15', '150', '16', '17', '18', '19', '1960', '1970', '1980', '1990', '20', '20 percent', '20 year', '200', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2016 elect', '2016 presidenti', '2017', '2018', '2019', '2020', '21', '21st', '21st centuri', '21wire', '21wiretv', '22', '23', '24', '25', '26', '27', '28', '29', '30', '300', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '400', '41', '42', '43', '44', '45', '46', '48', '49', '50', '500', '51', '52', '55', '60', '600', '65', '70', '75', '80', '800', '90', '911', 'abandon', 'abc', 'abil', 'abl', 'abort', 'abroad', 'absenc', 'absolut', 'absurd', 'abus', 'academ', 'academi', 'acceler', 'accept', 'access', 'accid', 'accompani', 'accomplish', 'accord', 'accord report', 'account', 'accur', 'accus', 'achiev', 'acknowledg', 'acquir', 'act', 'action', 'activ', 'activist', 'actor', 'actress', 'actual', 'ad', 'adam', 'add', 'addit', 'address', 'adjust', 'administr', 'administr offici', 'admir', 'admiss', 'admit', 'adopt', 'adult', 'advanc', 'advantag', 'advertis', 'advic', 'advis', 'advisor', 'advoc', 'advocaci', 'affair', 'affect', 'affili', 'afford', 'afford care', 'afghanistan', 'afraid', 'africa', 'african', 'africanamerican', 'aftermath', 'afternoon', 'afterward', 'age', 'agenc', 'agenc said', 'agenda', 'agent', 'aggress', 'ago', 'agre', 'agreement', 'agricultur', 'ahead', 'aid', 'aim', 'air', 'air forc', 'aircraft', 'airlin', 'airport', 'al', 'al qaeda', 'alabama', 'alarm', 'alert', 'alex', 'ali', 'alien', 'align', 'aliv', 'alleg', 'allegedli', 'alli', 'allianc', 'allow', 'alon', 'alongsid', 'alreadi', 'alter', 'altern', 'alway', 'amaz', 'ambassador', 'ambit', 'amend', 'america', 'america great', 'american', 'american citizen', 'american peopl', 'amid', 'analysi', 'analyst', 'anchor', 'andrew', 'angel', 'angela', 'angela merkel', 'anger', 'angri', 'anim', 'ann', 'announc', 'annual', 'anonym', 'anoth', 'answer', 'answer question', 'anthoni', 'anticip', 'antitrump', 'anybodi', 'anymor', 'anyon', 'anyth', 'anywher', 'apart', 'apolog', 'appar', 'appeal', 'appear', 'appl', 'appli', 'applic', 'appoint', 'appreci', 'approach', 'appropri', 'approv', 'approxim', 'april', 'arab', 'arabia', 'area', 'aren', 'arent', 'argu', 'argument', 'arizona', 'arm', 'armi', 'arrang', 'arrest', 'arriv', 'art', 'articl', 'artist', 'asia', 'asian', 'asid', 'ask', 'ask question', 'aspect', 'ass', 'assad', 'assassin', 'assault', 'assembl', 'assert', 'assess', 'asset', 'assign', 'assist', 'associ', 'associ press', 'assum', 'assur', 'asylum', 'athlet', 'atlant', 'atmospher', 'attach', 'attack', 'attempt', 'attend', 'attent', 'attitud', 'attorney', 'attorney gener', 'attract', 'attribut', 'audienc', 'audio', 'aug', 'august', 'australia', 'author', 'automat', 'avail', 'avenu', 'averag', 'avoid', 'await', 'awar', 'award', 'away', 'babi', 'background', 'backlash', 'bad', 'badli', 'bag', 'balanc', 'ball', 'ballist', 'ballist missil', 'ballot', 'ban', 'band', 'bank', 'banker', 'bannon', 'bar', 'barack', 'barack obama', 'bare', 'barrier', 'base', 'bashar', 'basi', 'basic', 'bathroom', 'battl', 'bay', 'beach', 'bear', 'beat', 'beauti', 'becam', 'becom', 'becom member', 'becom presid', 'bed', 'beg', 'began', 'begin', 'begun', 'behalf', 'behavior', 'beij', 'belief', 'believ', 'belong', 'ben', 'benefit', 'benghazi', 'benjamin', 'berlin', 'berni', 'berni sander', 'besid', 'best', 'bet', 'betray', 'better', 'bia', 'bid', 'big', 'bigger', 'biggest', 'bigot', 'bilater', 'billion', 'billion dollar', 'billionair', 'bipartisan', 'birth', 'bit', 'bitter', 'bizarr', 'black', 'black live', 'blame', 'blast', 'bless', 'blind', 'bloc', 'block', 'blog', 'blood', 'bloodi', 'blow', 'blue', 'board', 'boast', 'boat', 'bob', 'bodi', 'bolster', 'bomb', 'bond', 'book', 'boom', 'boost', 'border', 'born', 'boss', 'boston', 'bother', 'bought', 'bound', 'box', 'boy', 'brag', 'brain', 'branch', 'brand', 'brave', 'brazil', 'breach', 'break', 'breath', 'breitbart', 'breitbart news', 'brexit', 'brian', 'bridg', 'brief', 'briefli', 'brilliant', 'bring', 'britain', 'british', 'broad', 'broadcast', 'broader', 'broke', 'broken', 'brooklyn', 'brother', 'brought', 'brown', 'brussel', 'brutal', 'bu', 'budget', 'build', 'build wall', 'built', 'bullet', 'bulli', 'bunch', 'burden', 'bureau', 'buri', 'burn', 'bush', 'busi', 'businessman', 'bust', 'buy', 'cabinet', 'cabl', 'calcul', 'california', 'caller', 'calm', 'came', 'camera', 'camp', 'campaign', 'campaign manag', 'campaign promis', 'campaign trail', 'campu', 'canada', 'canadian', 'cancel', 'cancer', 'candid', 'candid donald', 'candid hillari', 'candidaci', 'cap', 'capabl', 'capac', 'capit', 'capitol', 'capitol hill', 'captur', 'car', 'card', 'care', 'care act', 'career', 'carolina', 'carri', 'carter', 'case', 'cash', 'cast', 'casualti', 'catastroph', 'catch', 'categori', 'cathol', 'caucu', 'caught', 'caus', 'caution', 'cb', 'celebr', 'cell', 'center', 'central', 'centuri', 'centuri wire', 'ceo', 'ceremoni', 'certain', 'certainli', 'chain', 'chair', 'chairman', 'challeng', 'chamber', 'champion', 'chanc', 'chancellor', 'chancellor angela', 'chang', 'channel', 'chant', 'chao', 'chapter', 'charact', 'character', 'charg', 'chariti', 'charl', 'chase', 'cheap', 'check', 'cheer', 'chemic', 'chicago', 'chief', 'chief execut', 'chief staff', 'child', 'children', 'china', 'chines', 'chip', 'choic', 'choos', 'chose', 'chosen', 'chri', 'christian', 'christma', 'chuck', 'church', 'cia', 'circl', 'circuit', 'circul', 'circumst', 'cite', 'citi', 'citizen', 'citizenship', 'civil', 'civil right', 'civil war', 'civilian', 'claim', 'clarifi', 'clash', 'class', 'classic', 'classifi', 'classifi inform', 'clean', 'clear', 'clearli', 'cleveland', 'click', 'client', 'climat', 'climat chang', 'climb', 'clinic', 'clinton', 'clinton campaign', 'clinton email', 'clinton foundat', 'clinton said', 'clip', 'close', 'closer', 'closest', 'cloth', 'club', 'cnn', 'coach', 'coal', 'coalit', 'coast', 'code', 'coincid', 'cold', 'cold war', 'collabor', 'collaps', 'colleagu', 'collect', 'colleg', 'collus', 'color', 'colorado', 'columbia', 'columnist', 'combat', 'combin', 'come', 'comey', 'comfort', 'command', 'comment', 'commentari', 'commerc', 'commerci', 'commiss', 'commission', 'commit', 'committe', 'committe chairman', 'committe said', 'common', 'commun', 'communist', 'compani', 'compar', 'comparison', 'compel', 'compet', 'competit', 'complain', 'complaint', 'complet', 'complex', 'compli', 'complic', 'compound', 'comprehens', 'compromis', 'comput', 'conced', 'concentr', 'concept', 'concern', 'concert', 'conclud', 'conclus', 'concret', 'condemn', 'condit', 'condit anonym', 'conduct', 'confer', 'confid', 'confirm', 'conflict', 'confront', 'confus', 'congress', 'congression', 'congressman', 'connect', 'consensu', 'consequ', 'conserv', 'consid', 'consider', 'consist', 'conspiraci', 'constant', 'constantli', 'constitu', 'constitut', 'construct', 'consult', 'consum', 'contact', 'contain', 'contend', 'content', 'contest', 'context', 'continu', 'contract', 'contractor', 'contradict', 'contrari', 'contrast', 'contribut', 'contributor', 'control', 'controversi', 'conveni', 'convent', 'convers', 'convict', 'convinc', 'cook', 'cool', 'cooper', 'coordin', 'cop', 'copi', 'core', 'corner', 'corp', 'corpor', 'correct', 'correspond', 'corrupt', 'cost', 'couldn', 'council', 'counsel', 'count', 'counter', 'counterpart', 'counti', 'countri', 'coup', 'coupl', 'courag', 'cours', 'court', 'court justic', 'court rule', 'cover', 'coverag', 'crack', 'crackdown', 'crash', 'crazi', 'creat', 'creation', 'creativ', 'credibl', 'credit', 'crew', 'cri', 'crime', 'crimin', 'crisi', 'critic', 'critic trump', 'crook', 'cross', 'crowd', 'crucial', 'crush', 'cruz', 'cuba', 'cultur', 'curb', 'currenc', 'current', 'custodi', 'custom', 'cut', 'cyber', 'cycl', 'daili', 'daili mail', 'dakota', 'damag', 'damn', 'dan', 'danc', 'danger', 'daniel', 'dare', 'dark', 'data', 'date', 'daughter', 'davi', 'david', 'day', 'dc', 'dead', 'deadli', 'deadlin', 'deal', 'death', 'debat', 'debt', 'dec', 'decad', 'decad ago', 'decemb', 'decid', 'decis', 'declar', 'declin', 'declin comment', 'dedic', 'deem', 'deep', 'deeper', 'deepli', 'defeat', 'defend', 'defens', 'defi', 'deficit', 'defin', 'definit', 'degre', 'delay', 'deleg', 'delet', 'deliber', 'deliv', 'dem', 'demand', 'democraci', 'democrat', 'democrat leader', 'democrat nation', 'democrat parti', 'democrat presid', 'democrat presidenti', 'democrat republican', 'democrat senat', 'demonstr', 'deni', 'denounc', 'depart', 'depart homeland', 'depart justic', 'depart said', 'departur', 'depend', 'depict', 'deploy', 'deport', 'depress', 'deputi', 'describ', 'descript', 'deserv', 'design', 'desir', 'desper', 'despit', 'destroy', 'destruct', 'detain', 'detect', 'detent', 'determin', 'detroit', 'devast', 'develop', 'devic', 'devot', 'dialogu', 'dictat', 'didn', 'didnt', 'die', 'differ', 'difficult', 'difficulti', 'dig', 'digit', 'dinner', 'diplomaci', 'diplomat', 'direct', 'directli', 'director', 'director jame', 'dirti', 'disabl', 'disagre', 'disappear', 'disappoint', 'disast', 'disclos', 'disclosur', 'discov', 'discredit', 'discrimin', 'discuss', 'diseas', 'disgrac', 'disgust', 'dismantl', 'dismiss', 'displac', 'display', 'disput', 'disrespect', 'disrupt', 'dissent', 'distanc', 'distinct', 'distract', 'distribut', 'district', 'disturb', 'divers', 'divid', 'divis', 'divorc', 'dnc', 'doctor', 'document', 'documentari', 'doesn', 'doesnt', 'dog', 'dollar', 'domest', 'domin', 'don', 'don know', 'don think', 'don want', 'donald', 'donald trump', 'donat', 'donor', 'dont', 'dont know', 'dont think', 'dont want', 'door', 'doubl', 'doubt', 'dozen', 'dr', 'draft', 'drag', 'drama', 'dramat', 'draw', 'drawn', 'dream', 'dress', 'drew', 'drill', 'drink', 'drive', 'driven', 'driver', 'drone', 'drop', 'drove', 'drug', 'dump', 'duti', 'eager', 'ear', 'earli', 'earlier', 'earlier month', 'earlier week', 'earlier year', 'earn', 'earth', 'eas', 'easi', 'easier', 'easili', 'east', 'eastern', 'eat', 'echo', 'econom', 'econom growth', 'economi', 'economist', 'edg', 'edit', 'editor', 'editori', 'educ', 'edward', 'effect', 'effici', 'effort', 'egypt', 'el', 'elabor', 'elect', 'elect campaign', 'elect day', 'elect offici', 'elect presid', 'elect trump', 'elector', 'electr', 'electron', 'element', 'elev', 'elig', 'elimin', 'elit', 'elizabeth', 'elsewher', 'email', 'email server', 'embarrass', 'embassi', 'embrac', 'emerg', 'emot', 'emphas', 'empir', 'employ', 'employe', 'empti', 'enabl', 'enact', 'encount', 'encourag', 'end', 'endors', 'endur', 'enemi', 'energi', 'enforc', 'engag', 'engin', 'england', 'english', 'enhanc', 'enjoy', 'enorm', 'ensur', 'enter', 'enterpris', 'entertain', 'entir', 'entiti', 'entitl', 'entri', 'environ', 'environment', 'episod', 'equal', 'equip', 'equival', 'era', 'eric', 'error', 'erupt', 'escal', 'escap', 'especi', 'essenti', 'establish', 'estat', 'estim', 'ethic', 'ethnic', 'eu', 'euro', 'europ', 'european', 'european union', 'evalu', 'event', 'eventu', 'everi', 'everi day', 'everi singl', 'everi time', 'everi year', 'everybodi', 'everyon', 'everyth', 'everywher', 'evid', 'evil', 'exact', 'exactli', 'examin', 'exampl', 'excel', 'excess', 'exchang', 'excit', 'exclud', 'exclus', 'excus', 'execut', 'execut director', 'execut order', 'exempt', 'exercis', 'exist', 'exit', 'expand', 'expans', 'expect', 'expens', 'experi', 'experienc', 'expert', 'expir', 'explain', 'explan', 'explod', 'exploit', 'explor', 'explos', 'export', 'expos', 'express', 'express concern', 'extend', 'extens', 'extent', 'extra', 'extraordinari', 'extrem', 'extremist', 'eye', 'fabric', 'face', 'facebook', 'facebook page', 'facil', 'fact', 'faction', 'factor', 'factori', 'fail', 'failur', 'fair', 'fairli', 'faith', 'fake', 'fake news', 'fall', 'fallen', 'fals', 'famili', 'famili member', 'familiar', 'famou', 'fan', 'far', 'farm', 'fashion', 'fast', 'fatal', 'fate', 'father', 'fault', 'favor', 'favorit', 'fbi', 'fbi director', 'fear', 'featur', 'featur imag', 'feb', 'februari', 'fed', 'feder', 'feder court', 'feder govern', 'feder judg', 'feder law', 'fee', 'feed', 'feel', 'feet', 'fell', 'fellow', 'felt', 'femal', 'fewer', 'field', 'fierc', 'fifth', 'fight', 'fighter', 'figur', 'file', 'film', 'final', 'financ', 'financi', 'fine', 'finger', 'finish', 'firearm', 'firm', 'fiscal', 'fish', 'fit', 'fix', 'flag', 'flaw', 'fled', 'flee', 'fli', 'flight', 'flip', 'flood', 'floor', 'florida', 'flow', 'flynn', 'focu', 'focus', 'folk', 'follow', 'follow twitter', 'food', 'foot', 'footag', 'footbal', 'forc', 'foreign', 'foreign minist', 'foreign ministri', 'foreign polici', 'forev', 'forget', 'form', 'formal', 'forth', 'fortun', 'forum', 'forward', 'fought', 'foundat', 'founder', 'fourth', 'fox', 'fox news', 'frame', 'franc', 'francisco', 'frank', 'frankli', 'fraud', 'free', 'free speech', 'free trade', 'freedom', 'french', 'frequent', 'fresh', 'friday', 'friend', 'friendli', 'frontrunn', 'frustrat', 'fuel', 'fulfil', 'fulli', 'fun', 'function', 'fund', 'fundament', 'fundrais', 'funni', 'futur', 'ga', 'gain', 'game', 'gang', 'gap', 'gari', 'gate', 'gather', 'gave', 'gay', 'gear', 'gender', 'gener', 'gener elect', 'genuin', 'georg', 'georg bush', 'georgia', 'german', 'germani', 'getti', 'getti imag', 'giant', 'gift', 'girl', 'given', 'glass', 'global', 'globe', 'goal', 'god', 'goe', 'gold', 'golden', 'golf', 'gon', 'gon na', 'gone', 'good', 'googl', 'gop', 'got', 'gotten', 'gov', 'govern', 'govern offici', 'govern said', 'governor', 'grab', 'graduat', 'graham', 'grand', 'grant', 'graphic', 'grave', 'great', 'greater', 'greatest', 'green', 'greet', 'grew', 'gross', 'ground', 'group', 'group said', 'grow', 'grown', 'growth', 'guarante', 'guard', 'guardian', 'guess', 'guest', 'guid', 'guilti', 'gulf', 'gun', 'guy', 'hack', 'hacker', 'hail', 'hair', 'half', 'hall', 'halt', 'hammer', 'hampshir', 'hand', 'handl', 'hang', 'happen', 'happi', 'harass', 'hard', 'harder', 'hardli', 'harm', 'harri', 'harsh', 'harvard', 'hasn', 'hat', 'hate', 'hatr', 'haven', 'havent', 'head', 'headlin', 'headquart', 'health', 'health care', 'health insur', 'healthcar', 'healthi', 'hear', 'heard', 'heart', 'heat', 'heavi', 'heavili', 'hedg', 'height', 'held', 'hell', 'help', 'heritag', 'hero', 'hey', 'hidden', 'hide', 'high', 'high school', 'higher', 'highest', 'highli', 'highlight', 'hilari', 'hill', 'hillari', 'hillari clinton', 'hint', 'hire', 'hispan', 'histor', 'histori', 'hit', 'hold', 'holder', 'hole', 'holi', 'holiday', 'hollywood', 'home', 'homeland', 'homeland secur', 'honest', 'honor', 'hope', 'horribl', 'horrif', 'hospit', 'host', 'hostil', 'hot', 'hotel', 'hour', 'hous', 'hous offici', 'hous press', 'hous repres', 'hous republican', 'hous said', 'hous senat', 'hous speaker', 'household', 'huge', 'human', 'human right', 'humanitarian', 'humili', 'hundr', 'hundr thousand', 'hunt', 'hurrican', 'hurt', 'husband', 'ice', 'id', 'idea', 'ideal', 'ident', 'identifi', 'ideolog', 'idiot', 'ignor', 'ii', 'ill', 'illeg', 'illeg alien', 'illeg immigr', 'illinoi', 'illustr', 'im', 'imag', 'imag video', 'imagin', 'immedi', 'immigr', 'immun', 'impact', 'impeach', 'implement', 'impli', 'implic', 'import', 'impos', 'imposs', 'impress', 'improv', 'inappropri', 'inaugur', 'incid', 'incit', 'includ', 'inclus', 'incom', 'increas', 'increasingli', 'incred', 'inde', 'independ', 'india', 'indian', 'indiana', 'indic', 'indict', 'individu', 'industri', 'inequ', 'inevit', 'infam', 'inflat', 'influenc', 'influenti', 'inform', 'infrastructur', 'initi', 'injur', 'injuri', 'inner', 'innoc', 'innov', 'inquiri', 'insan', 'insid', 'insist', 'inspir', 'instal', 'instanc', 'instead', 'institut', 'instruct', 'insult', 'insur', 'insurg', 'integr', 'intellectu', 'intellig', 'intellig agenc', 'intellig committe', 'intend', 'intens', 'intent', 'interact', 'interfer', 'interior', 'intern', 'internet', 'interpret', 'interven', 'intervent', 'interview', 'intimid', 'introduc', 'invad', 'invas', 'invest', 'investig', 'investor', 'invit', 'involv', 'iowa', 'iran', 'iranian', 'iraq', 'iraqi', 'iron', 'isi', 'islam', 'islam state', 'islamist', 'island', 'isn', 'isnt', 'isol', 'isra', 'israel', 'issu', 'itali', 'italian', 'item', 'ivanka', 'ive', 'jack', 'jail', 'jame', 'jame comey', 'jan', 'jan 20', 'januari', 'japan', 'japanes', 'jare', 'jason', 'jay', 'jeff', 'jeff session', 'jersey', 'jerusalem', 'jet', 'jew', 'jewish', 'jihadist', 'jim', 'jimmi', 'jinp', 'job', 'joe', 'john', 'john mccain', 'john podesta', 'johnson', 'join', 'joint', 'joke', 'jone', 'jordan', 'joseph', 'josh', 'journal', 'journalist', 'journey', 'joy', 'jr', 'judg', 'judgment', 'judici', 'judiciari', 'judiciari committe', 'juli', 'jump', 'june', 'juri', 'justic', 'justic depart', 'justifi', 'justin', 'kelli', 'kennedi', 'kentucki', 'kept', 'kerri', 'kevin', 'key', 'kick', 'kid', 'kill', 'killer', 'kim', 'kind', 'king', 'kingdom', 'km', 'knew', 'knock', 'know', 'knowledg', 'known', 'korea', 'korean', 'kremlin', 'kurdish', 'la', 'label', 'labor', 'lack', 'ladi', 'laid', 'land', 'languag', 'larg', 'larger', 'largest', 'lash', 'late', 'later', 'latest', 'latin', 'latino', 'laugh', 'launch', 'law', 'law enforc', 'lawmak', 'lawsuit', 'lawyer', 'lay', 'lead', 'leader', 'leader mitch', 'leadership', 'leagu', 'leak', 'lean', 'learn', 'leav', 'led', 'lee', 'left', 'leftist', 'leg', 'legaci', 'legal', 'legisl', 'legislatur', 'legitim', 'lesson', 'let', 'letter', 'level', 'liar', 'liber', 'liberti', 'libya', 'licens', 'lie', 'life', 'lifetim', 'lift', 'light', 'like', 'limit', 'line', 'link', 'list', 'listen', 'liter', 'littl', 'littl bit', 'live', 'live matter', 'll', 'lo', 'lo angel', 'load', 'loan', 'lobbi', 'lobbyist', 'local', 'locat', 'lock', 'logic', 'london', 'london reuter', 'long', 'long time', 'longer', 'longstand', 'longterm', 'longtim', 'look', 'look forward', 'lose', 'loss', 'lost', 'lot', 'lot peopl', 'love', 'low', 'lower', 'loyal', 'lynch', 'machin', 'mad', 'magazin', 'mail', 'main', 'mainli', 'mainstream', 'mainstream media', 'maintain', 'major', 'major leader', 'make', 'make america', 'make sens', 'make sure', 'maker', 'male', 'man', 'manag', 'mandat', 'manhattan', 'mani', 'mani peopl', 'mani year', 'manipul', 'manner', 'manufactur', 'map', 'march', 'marco', 'marco rubio', 'margin', 'mari', 'marin', 'mark', 'market', 'marri', 'marriag', 'martin', 'mass', 'massachusett', 'massacr', 'massiv', 'master', 'match', 'mate', 'materi', 'matt', 'matter', 'matthew', 'maximum', 'mayb', 'mayor', 'mccain', 'mcconnel', 'mean', 'meant', 'meanwhil', 'measur', 'mechan', 'meddl', 'media', 'media outlet', 'media report', 'medic', 'medicin', 'meet', 'member', 'member 21wiretv', 'member congress', 'membership', 'memo', 'memori', 'men', 'men women', 'mental', 'mention', 'mere', 'merkel', 'mess', 'messag', 'met', 'method', 'mexican', 'mexico', 'michael', 'michael flynn', 'michel', 'michigan', 'middl', 'middl east', 'migrant', 'migrat', 'mike', 'mike penc', 'mile', 'milit', 'militari', 'militia', 'miller', 'million', 'million american', 'million dollar', 'million peopl', 'mind', 'minimum', 'minist', 'minist theresa', 'ministri', 'ministri said', 'minnesota', 'minor', 'minut', 'mirror', 'miss', 'missil', 'mission', 'missouri', 'mistak', 'mitch', 'mitch mcconnel', 'mitt', 'mitt romney', 'mix', 'mobil', 'mock', 'model', 'moder', 'modern', 'mogul', 'moham', 'mom', 'moment', 'monday', 'money', 'monitor', 'month', 'month ago', 'moor', 'moral', 'morn', 'moscow', 'mosqu', 'mostli', 'mother', 'motion', 'motiv', 'mount', 'mountain', 'mouth', 'movement', 'movi', 'mr', 'mr clinton', 'mr obama', 'mr trump', 'ms', 'msnbc', 'multipl', 'murder', 'music', 'muslim', 'mutual', 'mysteri', 'na', 'nanci', 'narr', 'narrow', 'nation', 'nation committe', 'nation convent', 'nation secur', 'nationalist', 'nationwid', 'nativ', 'nato', 'natur', 'navi', 'nazi', 'nbc', 'near', 'nearbi', 'nearli', 'necessari', 'necessarili', 'need', 'neg', 'negoti', 'neighbor', 'neighborhood', 'net', 'network', 'neutral', 'nevada', 'new', 'new hampshir', 'new jersey', 'new presid', 'new york', 'newli', 'news', 'news 21st', 'news agenc', 'news confer', 'news media', 'news report', 'newspap', 'nice', 'night', 'nobodi', 'nomin', 'nomine', 'nomine donald', 'nonprofit', 'normal', 'north', 'north carolina', 'north korea', 'north korean', 'northern', 'notabl', 'note', 'noth', 'notic', 'notion', 'notori', 'nov', 'nov elect', 'novemb', 'novemb 2016', 'nuclear', 'nuclear weapon', 'number', 'numer', 'obama', 'obama administr', 'obama said', 'obamacar', 'object', 'oblig', 'observ', 'obsess', 'obstruct', 'obtain', 'obviou', 'obvious', 'occas', 'occasion', 'occup', 'occupi', 'occur', 'oct', 'octob', 'odd', 'offend', 'offens', 'offer', 'offic', 'offic said', 'offici', 'offici said', 'offici say', 'offici told', 'oh', 'ohio', 'oil', 'ok', 'okay', 'old', 'older', 'ongo', 'onlin', 'open', 'openli', 'oper', 'opinion', 'opinion poll', 'oppon', 'opportun', 'oppos', 'opposit', 'oppress', 'option', 'orang', 'order', 'ordinari', 'oregon', 'organ', 'origin', 'orlando', 'oust', 'outcom', 'outlet', 'outlin', 'outrag', 'outright', 'outsid', 'oval', 'oval offic', 'overal', 'overcom', 'overhaul', 'overse', 'oversea', 'oversight', 'overturn', 'overwhelm', 'overwhelmingli', 'owe', 'owner', 'pace', 'pacif', 'pack', 'packag', 'pact', 'page', 'paid', 'pain', 'paint', 'pair', 'pakistan', 'palestinian', 'panel', 'paper', 'paragraph', 'parent', 'pari', 'park', 'parliament', 'parliamentari', 'parti', 'parti leader', 'partial', 'particip', 'particular', 'particularli', 'partisan', 'partli', 'partner', 'partnership', 'pass', 'passag', 'passeng', 'passion', 'past', 'past year', 'path', 'patient', 'patrick', 'patriot', 'patrol', 'pattern', 'paul', 'paul ryan', 'pay', 'payment', 'peac', 'pen', 'penalti', 'penc', 'pend', 'peninsula', 'pennsylvania', 'pentagon', 'peopl', 'peopl kill', 'peopl live', 'peopl said', 'peopl vote', 'peopl want', 'perceiv', 'percent', 'percentag', 'percept', 'perfect', 'perfectli', 'perform', 'perhap', 'period', 'perman', 'permiss', 'permit', 'persecut', 'persist', 'person', 'personnel', 'perspect', 'persuad', 'peter', 'petit', 'phase', 'philadelphia', 'phone', 'photo', 'photograph', 'phrase', 'physic', 'pick', 'pictur', 'piec', 'pipelin', 'place', 'plan', 'plane', 'planet', 'plant', 'platform', 'play', 'player', 'plead', 'pleas', 'pledg', 'plenti', 'plot', 'plu', 'pm', 'pocket', 'podesta', 'point', 'poison', 'polic', 'polic depart', 'polic offic', 'polic said', 'polici', 'polit', 'polit parti', 'politician', 'politico', 'poll', 'pool', 'poor', 'pop', 'popul', 'popular', 'populist', 'port', 'portion', 'portray', 'pose', 'posit', 'possess', 'possibl', 'post', 'potenti', 'potu', 'pound', 'pour', 'poverti', 'power', 'practic', 'prais', 'pray', 'prayer', 'preced', 'precis', 'predecessor', 'predict', 'prefer', 'premium', 'prepar', 'presenc', 'present', 'preserv', 'presid', 'presid barack', 'presid bashar', 'presid clinton', 'presid donald', 'presid georg', 'presid obama', 'presid said', 'presid trump', 'presid unit', 'presid vladimir', 'presidentelect', 'presidentelect donald', 'presidenti', 'presidenti campaign', 'presidenti candid', 'presidenti elect', 'presidenti nomine', 'presidenti race', 'press', 'press confer', 'press secretari', 'pressur', 'presum', 'presumpt', 'pretend', 'pretti', 'prevent', 'previou', 'previous', 'price', 'pride', 'primari', 'primarili', 'prime', 'prime minist', 'princip', 'principl', 'print', 'prior', 'prioriti', 'prison', 'privaci', 'privat', 'privat email', 'privileg', 'prize', 'probabl', 'probe', 'problem', 'procedur', 'proceed', 'process', 'produc', 'product', 'profession', 'professor', 'profil', 'profit', 'program', 'progress', 'prohibit', 'project', 'promin', 'promis', 'promot', 'prompt', 'proof', 'propaganda', 'proper', 'properli', 'properti', 'propos', 'prosecut', 'prosecutor', 'prospect', 'prosper', 'protect', 'protest', 'proud', 'prove', 'proven', 'provid', 'provinc', 'provis', 'provoc', 'provok', 'psycholog', 'public', 'publicli', 'publish', 'pull', 'punch', 'pundit', 'punish', 'purchas', 'pure', 'purpos', 'pursu', 'push', 'putin', 'pyongyang', 'qaeda', 'qualifi', 'qualiti', 'quarter', 'queen', 'question', 'quick', 'quickli', 'quiet', 'quietli', 'quit', 'quot', 'race', 'racial', 'racism', 'racist', 'radic', 'radio', 'rage', 'raid', 'rail', 'rain', 'rais', 'rais question', 'ralli', 'ran', 'rang', 'rank', 'rant', 'rape', 'rapid', 'rapidli', 'rapist', 'rare', 'rate', 'reach', 'react', 'reaction', 'read', 'reader', 'readi', 'reagan', 'real', 'real estat', 'realdonaldtrump', 'realiti', 'realiz', 'realli', 'reason', 'reassur', 'rebel', 'rebuild', 'recal', 'receiv', 'recent', 'recent month', 'recent week', 'recent year', 'recess', 'recogn', 'recognit', 'recommend', 'record', 'recount', 'recov', 'recruit', 'red', 'reduc', 'reduct', 'reelect', 'refer', 'referendum', 'reflect', 'reform', 'refuge', 'refus', 'regard', 'regardless', 'regim', 'region', 'regist', 'regret', 'regul', 'regular', 'regularli', 'regulatori', 'reinforc', 'reiter', 'reject', 'rel', 'relat', 'relationship', 'releas', 'relev', 'reli', 'reliabl', 'relief', 'religi', 'religion', 'reluct', 'remain', 'remark', 'rememb', 'remind', 'remot', 'remov', 'renew', 'rent', 'reopen', 'rep', 'repeal', 'repeat', 'repeatedli', 'replac', 'repli', 'report', 'report said', 'reportedli', 'repres', 'republ', 'republican', 'republican candid', 'republican democrat', 'republican lawmak', 'republican leader', 'republican nation', 'republican nomine', 'republican parti', 'republican presid', 'republican presidenti', 'republican senat', 'reput', 'request', 'request comment', 'requir', 'rescu', 'research', 'reserv', 'resid', 'resign', 'resist', 'resolut', 'resolv', 'resort', 'resourc', 'respect', 'respond', 'respond request', 'respons', 'rest', 'restaur', 'restor', 'restrict', 'result', 'resum', 'retain', 'retali', 'retir', 'retreat', 'return', 'reuter', 'reuter presid', 'reuter republican', 'reveal', 'revel', 'revenu', 'revers', 'review', 'revis', 'reviv', 'revolut', 'revolutionari', 'reward', 'rex', 'rex tillerson', 'rhetor', 'rich', 'richard', 'rick', 'rid', 'ride', 'ridicul', 'rifl', 'rig', 'right', 'right group', 'rightw', 'ring', 'riot', 'rip', 'rise', 'risk', 'rival', 'river', 'road', 'rob', 'robert', 'rock', 'roger', 'role', 'roll', 'romney', 'ron', 'ronald', 'room', 'root', 'rose', 'roughli', 'round', 'rout', 'routin', 'row', 'royal', 'rubio', 'ruin', 'rule', 'rule law', 'rumor', 'run', 'run presid', 'rural', 'rush', 'russia', 'russian', 'russian govern', 'russian presid', 'ryan', 'sad', 'safe', 'safeti', 'said', 'said ad', 'said believ', 'said email', 'said friday', 'said hope', 'said interview', 'said mani', 'said monday', 'said mr', 'said peopl', 'said plan', 'said presid', 'said refer', 'said said', 'said statement', 'said sunday', 'said think', 'said thursday', 'said time', 'said trump', 'said tuesday', 'said twitter', 'said want', 'said wednesday', 'said week', 'salari', 'sale', 'san', 'san francisco', 'sanction', 'sander', 'sarah', 'sat', 'satisfi', 'saturday', 'saudi', 'saudi arabia', 'save', 'saw', 'say', 'say trump', 'say want', 'scale', 'scandal', 'scare', 'scenario', 'scene', 'schedul', 'scheme', 'school', 'schumer', 'scienc', 'scientif', 'scientist', 'scope', 'score', 'scott', 'scrap', 'scream', 'screen', 'screen captur', 'screenshot', 'script', 'scrutini', 'sea', 'seal', 'sean', 'sean spicer', 'search', 'season', 'seat', 'second', 'secret', 'secretari', 'secretari state', 'section', 'sector', 'secur', 'secur advis', 'secur council', 'secur forc', 'seek', 'seemingli', 'seen', 'segment', 'seiz', 'select', 'sell', 'sen', 'senat', 'senat democrat', 'senat john', 'senat major', 'senat republican', 'send', 'senior', 'senior offici', 'sens', 'sensit', 'sent', 'sentenc', 'sentiment', 'separ', 'sept', 'septemb', 'seri', 'seriou', 'serv', 'server', 'servic', 'session', 'set', 'settl', 'settlement', 'seven', 'sever', 'sever year', 'sex', 'sexual', 'sexual assault', 'shadow', 'shake', 'shame', 'shape', 'share', 'sharp', 'sharpli', 'shed', 'shelter', 'sheriff', 'shi', 'shield', 'shift', 'ship', 'shock', 'shoot', 'shop', 'short', 'shortli', 'shot', 'shouldn', 'shouldnt', 'shout', 'shown', 'shut', 'sick', 'sight', 'sign', 'signal', 'signatur', 'signific', 'significantli', 'silenc', 'silent', 'silver', 'similar', 'simpl', 'simpli', 'sing', 'singer', 'singl', 'sink', 'sister', 'sit', 'site', 'situat', 'size', 'skeptic', 'skill', 'skin', 'sky', 'slam', 'slash', 'sleep', 'slightli', 'slip', 'slogan', 'slow', 'slowli', 'small', 'smaller', 'smart', 'smile', 'smith', 'smoke', 'socal', 'social', 'social media', 'socialist', 'societi', 'sold', 'soldier', 'sole', 'solid', 'solut', 'solv', 'somebodi', 'someon', 'someth', 'sometim', 'somewhat', 'son', 'song', 'soon', 'soro', 'sorri', 'sort', 'sought', 'soul', 'sound', 'sourc', 'sourc said', 'south', 'south carolina', 'south korea', 'southeast', 'southern', 'sovereignti', 'soviet', 'space', 'spain', 'spanish', 'spark', 'speak', 'speaker', 'speaker paul', 'special', 'special counsel', 'specif', 'specul', 'speech', 'speed', 'spend', 'spent', 'spi', 'spicer', 'spin', 'spirit', 'split', 'spoke', 'spoken', 'spokesman', 'spokesman said', 'spokesperson', 'spokeswoman', 'sponsor', 'sport', 'spot', 'spread', 'spring', 'squar', 'st', 'stabil', 'stabl', 'staff', 'staffer', 'stage', 'stake', 'stall', 'stanc', 'stand', 'standard', 'star', 'start', 'state', 'state depart', 'state govern', 'state rex', 'state said', 'state senat', 'state trump', 'statement', 'statement said', 'station', 'statist', 'statu', 'stay', 'steal', 'stem', 'step', 'stephen', 'steve', 'steven', 'stick', 'stir', 'stock', 'stolen', 'stone', 'stood', 'stop', 'store', 'stori', 'storm', 'straight', 'strain', 'strang', 'strateg', 'strategi', 'strategist', 'stream', 'street', 'street journal', 'strength', 'strengthen', 'stress', 'stretch', 'strike', 'string', 'strip', 'strong', 'stronger', 'strongli', 'struck', 'structur', 'struggl', 'stuck', 'student', 'studi', 'stuff', 'stun', 'stupid', 'style', 'su', 'subject', 'submit', 'subscrib', 'subscrib becom', 'subsequ', 'subsidi', 'substanti', 'succeed', 'success', 'successor', 'sudden', 'suddenli', 'sue', 'suffer', 'suffici', 'suggest', 'suicid', 'suit', 'sum', 'summer', 'summit', 'sun', 'sunday', 'sunni', 'super', 'suppli', 'support', 'support trump', 'suppos', 'supposedli', 'suppress', 'suprem', 'suprem court', 'supremacist', 'sure', 'surfac', 'surg', 'surpris', 'surround', 'surveil', 'survey', 'surviv', 'susan', 'suspect', 'suspend', 'suspici', 'suspicion', 'sustain', 'sweep', 'swing', 'switch', 'sworn', 'symbol', 'syria', 'syrian', 'tabl', 'tackl', 'tactic', 'tag', 'taken', 'talent', 'talk', 'tank', 'tap', 'tape', 'target', 'task', 'taught', 'tax', 'tax cut', 'taxpay', 'tea', 'teach', 'teacher', 'team', 'tear', 'tech', 'technic', 'technolog', 'ted', 'ted cruz', 'teenag', 'tehran', 'telephon', 'televis', 'tell', 'temporari', 'temporarili', 'tend', 'tension', 'tenur', 'term', 'termin', 'terribl', 'terrifi', 'territori', 'terror', 'terrorist', 'terrorist attack', 'test', 'testifi', 'testimoni', 'texa', 'text', 'thank', 'theater', 'theme', 'theori', 'theresa', 'theyr', 'theyv', 'thing', 'think', 'think tank', 'thoma', 'thought', 'thousand', 'thousand peopl', 'threat', 'threaten', 'threw', 'throw', 'thrown', 'thu', 'thursday', 'ticket', 'tie', 'tight', 'tighten', 'tillerson', 'tim', 'time', 'time report', 'timeswashington', 'tini', 'tip', 'tire', 'titl', 'today', 'togeth', 'told', 'told news', 'told report', 'told reuter', 'toler', 'toll', 'tom', 'tomorrow', 'tone', 'tonight', 'took', 'took offic', 'took place', 'tool', 'topic', 'tortur', 'total', 'touch', 'tough', 'tougher', 'tour', 'tourist', 'tower', 'town', 'track', 'trade', 'trade agreement', 'trade deal', 'tradit', 'traffic', 'tragedi', 'trail', 'train', 'transcript', 'transfer', 'transform', 'transgend', 'transit', 'transit team', 'translat', 'transpar', 'transport', 'trap', 'trash', 'travel', 'treasuri', 'treat', 'treati', 'treatment', 'tree', 'tremend', 'trend', 'tri', 'tri make', 'trial', 'trigger', 'trillion', 'trip', 'troop', 'troubl', 'truck', 'true', 'truli', 'trump', 'trump administr', 'trump campaign', 'trump elect', 'trump make', 'trump new', 'trump plan', 'trump presid', 'trump promis', 'trump ralli', 'trump realdonaldtrump', 'trump republican', 'trump said', 'trump say', 'trump support', 'trump told', 'trump took', 'trump tower', 'trump tweet', 'trump victori', 'trump want', 'trump white', 'trump win', 'trump won', 'trust', 'truth', 'tuesday', 'tune', 'turkey', 'turkish', 'turn', 'tv', 'tweet', 'twice', 'twist', 'twitter', 'twitter account', 'type', 'typic', 'ugli', 'uk', 'ukrain', 'ultim', 'unabl', 'unaccept', 'unanim', 'uncertainti', 'unclear', 'unconstitut', 'uncov', 'undermin', 'underscor', 'understand', 'understood', 'undocu', 'unemploy', 'unfair', 'unfold', 'unfortun', 'uniform', 'union', 'uniqu', 'unit', 'unit nation', 'unit state', 'uniti', 'univers', 'unknown', 'unleash', 'unless', 'unlik', 'unpreced', 'unusu', 'unveil', 'upcom', 'updat', 'upper', 'upset', 'urban', 'urg', 'urgent', 'usa', 'use', 'use privat', 'user', 'usual', 'util', 'vacat', 'valid', 'valley', 'valu', 'van', 'varieti', 'variou', 'vast', 've', 'vehicl', 'ventur', 'verifi', 'vermont', 'version', 'vet', 'veteran', 'veto', 'vice', 'vice presid', 'victim', 'victori', 'video', 'video screen', 'vietnam', 'view', 'viewer', 'villag', 'violat', 'violenc', 'violent', 'viral', 'virginia', 'virtual', 'visa', 'visibl', 'vision', 'visit', 'visitor', 'vital', 'vladimir', 'vladimir putin', 'vocal', 'voic', 'volunt', 'vote', 'vote trump', 'voter', 'vow', 'vulner', 'wage', 'wait', 'wake', 'walk', 'wall', 'wall street', 'want', 'want know', 'want make', 'war', 'warm', 'warn', 'warrant', 'warren', 'washington', 'washington dc', 'washington post', 'washington reuter', 'wasn', 'wasnt', 'wast', 'watch', 'watchdog', 'water', 'wave', 'way', 'weak', 'weaken', 'wealth', 'wealthi', 'weapon', 'wear', 'weather', 'web', 'websit', 'wed', 'wednesday', 'week', 'week ago', 'weekend', 'weekli', 'weigh', 'weight', 'welcom', 'welfar', 'went', 'west', 'western', 'weve', 'whatev', 'whenev', 'whine', 'white', 'white hous', 'white supremacist', 'wide', 'widespread', 'wife', 'wikileak', 'wild', 'william', 'win', 'win elect', 'wind', 'window', 'wing', 'winner', 'winter', 'wire', 'wire say', 'wisconsin', 'wish', 'wit', 'withdraw', 'woman', 'women', 'won', 'wonder', 'wont', 'word', 'work', 'work hard', 'work togeth', 'worker', 'world', 'world war', 'worldwid', 'worri', 'wors', 'worst', 'worth', 'wouldn', 'wouldnt', 'wound', 'wow', 'wrap', 'write', 'writer', 'written', 'wrong', 'wrongdo', 'wrote', 'xi', 'xi jinp', 'ye', 'yeah', 'year', 'year ago', 'year later', 'year old', 'year said', 'yell', 'yemen', 'yesterday', 'york', 'york citi', 'york reuter', 'york time', 'york timeswashington', 'youll', 'young', 'young peopl', 'younger', 'youth', 'youtub', 'youv', 'zero', 'zone']\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.01)\n",
    "tfidf_vectorizer.fit(X_train_text)\n",
    "list1 = list(tfidf_vectorizer.get_feature_names())\n",
    "print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.00)\n",
    "# tfidf_vectorizer.fit(X_train_text)\n",
    "# list2 = list(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique = set(list2) - set(list1)\n",
    "# print(len(list(unique)))\n",
    "# print(list(unique)[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n",
      "['2016', 'accord', 'act', 'ad', 'administr', 'allow', 'america', 'american', 'anoth', 'appear', 'ask', 'attack', 'becom', 'believ', 'campaign', 'case', 'chang', 'citi', 'claim', 'clinton', 'close', 'come', 'comment', 'continu', 'countri', 'critic', 'day', 'democrat', 'donald', 'donald trump', 'elect', 'end', 'everi', 'face', 'fact', 'feder', 'follow', 'forc', 'gener', 'good', 'govern', 'group', 'happen', 'help', 'hillari', 'hillari clinton', 'hous', 'imag', 'includ', 'issu', 'know', 'law', 'lead', 'leader', 'live', 'long', 'look', 'major', 'make', 'mani', 'mean', 'media', 'meet', 'member', 'million', 'month', 'nation', 'need', 'new', 'new york', 'news', 'number', 'obama', 'offic', 'offici', 'order', 'parti', 'peopl', 'person', 'place', 'plan', 'point', 'polici', 'polit', 'possibl', 'post', 'power', 'presid', 'presidenti', 'public', 'question', 'realli', 'recent', 'report', 'repres', 'republican', 'respons', 'reuter', 'right', 'run', 'said', 'say', 'secur', 'senat', 'sever', 'start', 'state', 'statement', 'support', 'talk', 'thing', 'think', 'time', 'told', 'took', 'tri', 'trump', 'tuesday', 'turn', 'unit', 'unit state', 'use', 'video', 'vote', 'want', 'washington', 'way', 'week', 'white', 'white hous', 'work', 'world', 'year', 'york']\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.15)\n",
    "tfidf_vectorizer.fit(X_train_text)\n",
    "list3 = list(tfidf_vectorizer.get_feature_names())\n",
    "print(len(list3))\n",
    "print(list3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection for Additional Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Added Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = pd.read_csv(\"Final datasets/train_data.csv\")\n",
    "# val_data_features = pd.read_csv(\"Final datasets/val_data.csv\")\n",
    "test_data_features = pd.read_csv(\"Final datasets/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data_features[\"class_label\"].values\n",
    "y_test = test_data_features[\"class_label\"].values\n",
    "# y_val = val_data_features[\"class_label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All added features for min_df = 0.01 (3k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise TfidfVectorizer with min_df = 0.01 as per feature selection\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1,2), min_df=0.01)\n",
    "\n",
    "# Create mapper object to combine added features and tfidf word vectors\n",
    "mapper = DataFrameMapper([\n",
    "    (['char_count', 'word_count', 'sentence_count', 'prop_unique_words',\n",
    "    'avg_sentence_length', 'prop_punctuations', 'prop_stopwords',\n",
    "    'prop_words_in_quotes', 'prop_nouns', 'prop_verbs', 'prop_adjectives',\n",
    "    'prop_discourse_relations', 'textblob_sentiment'], None), \n",
    "    ('text_preprocessed', tfidf_vectorizer)\n",
    "])\n",
    "\n",
    "# fit_transform mapper on train data with added features and transform test data with added features\n",
    "X_train_added_features = mapper.fit_transform(train_data_features)\n",
    "X_test_added_features = mapper.transform(test_data_features)\n",
    "# X_val_added_features = mapper.transform(val_data_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define svm linearsvc model\n",
    "svm_clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      6361\n",
      "           1       0.96      0.97      0.96      6660\n",
      "\n",
      "    accuracy                           0.96     13021\n",
      "   macro avg       0.96      0.96      0.96     13021\n",
      "weighted avg       0.96      0.96      0.96     13021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_clf.fit(X_train_added_features, y_train)\n",
    "y_pred = svm_clf.predict(X_test_added_features)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected added features for min_df = 0.01 (3k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      6361\n",
      "           1       0.97      0.95      0.96      6660\n",
      "\n",
      "    accuracy                           0.96     13021\n",
      "   macro avg       0.96      0.96      0.96     13021\n",
      "weighted avg       0.96      0.96      0.96     13021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapper = DataFrameMapper([\n",
    "    (['char_count', 'word_count', 'prop_unique_words', 'avg_sentence_length', 'prop_punctuations', 'prop_stopwords', 'prop_nouns'], None), \n",
    "    ('text_preprocessed', tfidf_vectorizer)\n",
    "])\n",
    "\n",
    "# fit_transform mapper on train data with added features and transform test data with added features\n",
    "X_train_added_features = mapper.fit_transform(train_data_features)\n",
    "X_test_added_features = mapper.transform(test_data_features)\n",
    "\n",
    "svm_clf.fit(X_train_added_features, y_train)\n",
    "y_pred = svm_clf.predict(X_test_added_features)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All added features for min_df = 0.15 (134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90      6361\n",
      "           1       0.87      0.96      0.92      6660\n",
      "\n",
      "    accuracy                           0.91     13021\n",
      "   macro avg       0.92      0.91      0.91     13021\n",
      "weighted avg       0.91      0.91      0.91     13021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialise TfidfVectorizer with min_df = 0.01 as per feature selection\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1,2), min_df=0.15)\n",
    "\n",
    "# Create mapper object to combine added features and tfidf word vectors\n",
    "mapper = DataFrameMapper([\n",
    "    (['char_count', 'word_count', 'sentence_count', 'prop_unique_words',\n",
    "    'avg_sentence_length', 'prop_punctuations', 'prop_stopwords',\n",
    "    'prop_words_in_quotes', 'prop_nouns', 'prop_verbs', 'prop_adjectives',\n",
    "    'prop_discourse_relations', 'textblob_sentiment'], None), \n",
    "    ('text_preprocessed', tfidf_vectorizer)\n",
    "])\n",
    "\n",
    "# fit_transform mapper on train data with added features and transform test data with added features\n",
    "X_train_added_features = mapper.fit_transform(train_data_features)\n",
    "X_test_added_features = mapper.transform(test_data_features)\n",
    "\n",
    "svm_clf.fit(X_train_added_features, y_train)\n",
    "y_pred = svm_clf.predict(X_test_added_features)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected added features for min_df = 0.15 (134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89      6361\n",
      "           1       0.95      0.83      0.89      6660\n",
      "\n",
      "    accuracy                           0.89     13021\n",
      "   macro avg       0.90      0.89      0.89     13021\n",
      "weighted avg       0.90      0.89      0.89     13021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapper = DataFrameMapper([\n",
    "    (['char_count', 'word_count', 'prop_unique_words', 'avg_sentence_length', 'prop_punctuations', 'prop_stopwords', 'prop_nouns'], None), \n",
    "    ('text_preprocessed', tfidf_vectorizer)\n",
    "])\n",
    "\n",
    "# fit_transform mapper on train data with added features and transform test data with added features\n",
    "X_train_added_features = mapper.fit_transform(train_data_features)\n",
    "X_test_added_features = mapper.transform(test_data_features)\n",
    "\n",
    "svm_clf.fit(X_train_added_features, y_train)\n",
    "y_pred = svm_clf.predict(X_test_added_features)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "729bae39908f34aae3ae4cd67793570f1853d7623135d20860991de8be7ba981"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
