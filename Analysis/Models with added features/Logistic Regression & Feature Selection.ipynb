{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Packages for data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# Packages for machine learning modelling\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "# precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Packages for visualisation \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Packages for NLP\n",
    "import nltk\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"new data/train_data.csv\", index_col=1)\n",
    "val_data = pd.read_csv(\"new data/validation_data.csv\", index_col=1)\n",
    "test_data = pd.read_csv(\"new data/test_data.csv\", index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = train_data[\"text_preprocessed\"].values\n",
    "y_train = train_data[\"class_label\"].values\n",
    "\n",
    "X_val_text = val_data[\"text_preprocessed\"].values\n",
    "y_val = val_data[\"class_label\"].values\n",
    "\n",
    "X_test_text = test_data[\"text_preprocessed\"].values\n",
    "y_test = test_data[\"class_label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,1))\n",
    "vectorizer.fit(X_train_text)\n",
    "\n",
    "X_train = vectorizer.transform(X_train_text)\n",
    "X_val = vectorizer.transform(X_val_text)\n",
    "X_test = vectorizer.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features used: 238266\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features used:\", len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# Sparse vector of frequency of each word appearing in a text article\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_clf = LogisticRegression()\n",
    "log_reg_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Model with unigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97      6361\n",
      "           1       0.96      0.98      0.97      6659\n",
      "\n",
      "    accuracy                           0.97     13020\n",
      "   macro avg       0.97      0.97      0.97     13020\n",
      "weighted avg       0.97      0.97      0.97     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97      6361\n",
      "           1       0.97      0.97      0.97      6660\n",
      "\n",
      "    accuracy                           0.97     13021\n",
      "   macro avg       0.97      0.97      0.97     13021\n",
      "weighted avg       0.97      0.97      0.97     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "CountVectorizer Model with unigram and bigram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      6361\n",
      "           1       0.97      0.98      0.97      6659\n",
      "\n",
      "    accuracy                           0.97     13020\n",
      "   macro avg       0.97      0.97      0.97     13020\n",
      "weighted avg       0.97      0.97      0.97     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      6361\n",
      "           1       0.97      0.98      0.97      6660\n",
      "\n",
      "    accuracy                           0.97     13021\n",
      "   macro avg       0.97      0.97      0.97     13021\n",
      "weighted avg       0.97      0.97      0.97     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "CountVectorizer Model with bigram\n",
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95      6361\n",
      "           1       0.94      0.97      0.96      6659\n",
      "\n",
      "    accuracy                           0.95     13020\n",
      "   macro avg       0.96      0.95      0.95     13020\n",
      "weighted avg       0.96      0.95      0.95     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95      6361\n",
      "           1       0.94      0.97      0.96      6660\n",
      "\n",
      "    accuracy                           0.95     13021\n",
      "   macro avg       0.95      0.95      0.95     13021\n",
      "weighted avg       0.95      0.95      0.95     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer_params = {'unigram':(1,1), 'unigram and bigram': (1,2), 'bigram':(2,2)}\n",
    "\n",
    "for ngram, values in count_vectorizer_params.items():\n",
    "    vectorizer = CountVectorizer(stop_words='english', ngram_range=values)\n",
    "    vectorizer.fit(X_train_text)\n",
    "\n",
    "    X_train = vectorizer.transform(X_train_text)\n",
    "    X_val = vectorizer.transform(X_val_text)\n",
    "    X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "    print(f'CountVectorizer Model with {ngram}')\n",
    "    log_reg_clf.fit(X_train, y_train)\n",
    "\n",
    "    #Validation Data\n",
    "    print('Testing with validation data:')\n",
    "    val_pred = log_reg_clf.predict(X_val)\n",
    "    print(classification_report(y_val, val_pred))\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "    # Test Data\n",
    "    print('Testing using test data:')\n",
    "    test_pred = log_reg_clf.predict(X_test)\n",
    "    print(classification_report(y_test, test_pred))\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with unigram\n",
      "Testing using validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96      6361\n",
      "           1       0.95      0.96      0.96      6659\n",
      "\n",
      "    accuracy                           0.96     13020\n",
      "   macro avg       0.96      0.96      0.96     13020\n",
      "weighted avg       0.96      0.96      0.96     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95      6361\n",
      "           1       0.95      0.96      0.96      6660\n",
      "\n",
      "    accuracy                           0.95     13021\n",
      "   macro avg       0.95      0.95      0.95     13021\n",
      "weighted avg       0.95      0.95      0.95     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "Model with unigram and bigram\n",
      "Testing using validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      6361\n",
      "           1       0.96      0.95      0.96      6659\n",
      "\n",
      "    accuracy                           0.96     13020\n",
      "   macro avg       0.96      0.96      0.96     13020\n",
      "weighted avg       0.96      0.96      0.96     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      6361\n",
      "           1       0.96      0.95      0.95      6660\n",
      "\n",
      "    accuracy                           0.95     13021\n",
      "   macro avg       0.95      0.95      0.95     13021\n",
      "weighted avg       0.95      0.95      0.95     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "Model with bigram\n",
      "Testing using validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94      6361\n",
      "           1       0.94      0.95      0.94      6659\n",
      "\n",
      "    accuracy                           0.94     13020\n",
      "   macro avg       0.94      0.94      0.94     13020\n",
      "weighted avg       0.94      0.94      0.94     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94      6361\n",
      "           1       0.93      0.95      0.94      6660\n",
      "\n",
      "    accuracy                           0.94     13021\n",
      "   macro avg       0.94      0.94      0.94     13021\n",
      "weighted avg       0.94      0.94      0.94     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tfidf_params = {'unigram':(1,1), 'unigram and bigram': (1,2), 'bigram':(2,2)}\n",
    "\n",
    "for ngram, values in tfidf_params.items():\n",
    "    tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=values)\n",
    "    tfidf_vectorizer.fit(X_train_text)\n",
    "\n",
    "    X_train = tfidf_vectorizer.transform(X_train_text)\n",
    "    X_val = tfidf_vectorizer.transform(X_val_text)\n",
    "    X_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "    log_reg_clf = LogisticRegression()\n",
    "    print(f\"Model with {ngram}\")\n",
    "    log_reg_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Validation Data\n",
    "    print(\"Testing using validation data:\")    \n",
    "    y_val_pred = log_reg_clf.predict(X_val)\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "    # Test Data\n",
    "    print(\"Testing using test data:\")\n",
    "    y_test_pred = log_reg_clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_df = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------CountVectorizer--------------------\n",
      "CountVectorizer Model with min_df=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3373\n",
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96      6361\n",
      "           1       0.95      0.97      0.96      6659\n",
      "\n",
      "    accuracy                           0.96     13020\n",
      "   macro avg       0.96      0.96      0.96     13020\n",
      "weighted avg       0.96      0.96      0.96     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96      6361\n",
      "           1       0.96      0.96      0.96      6660\n",
      "\n",
      "    accuracy                           0.96     13021\n",
      "   macro avg       0.96      0.96      0.96     13021\n",
      "weighted avg       0.96      0.96      0.96     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------CountVectorizer--------------------')\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.01)\n",
    "vectorizer.fit(X_train_text)\n",
    "\n",
    "X_train = vectorizer.transform(X_train_text)\n",
    "X_val = vectorizer.transform(X_val_text)\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f'CountVectorizer Model with min_df=0.01')\n",
    "log_reg_clf.fit(X_train, y_train)\n",
    "num_features = len(vectorizer.get_feature_names())\n",
    "print(num_features)\n",
    "# countvectorizer_numfeatures.append(num_features)\n",
    "\n",
    "#Validation Data\n",
    "print('Testing with validation data:')\n",
    "val_pred = log_reg_clf.predict(X_val)\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Test Data\n",
    "print('Testing using test data:')\n",
    "test_pred = log_reg_clf.predict(X_test)\n",
    "report = classification_report(y_test, test_pred)\n",
    "print(report)\n",
    "print(\"------------------------------------------\")\n",
    "print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_df = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------CountVectorizer--------------------\n",
      "CountVectorizer Model with min_df=0.15\n",
      "134\n",
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.90      6361\n",
      "           1       0.89      0.94      0.91      6659\n",
      "\n",
      "    accuracy                           0.91     13020\n",
      "   macro avg       0.91      0.91      0.91     13020\n",
      "weighted avg       0.91      0.91      0.91     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91      6361\n",
      "           1       0.89      0.94      0.92      6660\n",
      "\n",
      "    accuracy                           0.91     13021\n",
      "   macro avg       0.91      0.91      0.91     13021\n",
      "weighted avg       0.91      0.91      0.91     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# values = [x * 0.01 for x in range(0, 16)]\n",
    "\n",
    "print('--------------------CountVectorizer--------------------')\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.15)\n",
    "vectorizer.fit(X_train_text)\n",
    "\n",
    "X_train = vectorizer.transform(X_train_text)\n",
    "X_val = vectorizer.transform(X_val_text)\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f'CountVectorizer Model with min_df=0.15')\n",
    "log_reg_clf.fit(X_train, y_train)\n",
    "num_features = len(vectorizer.get_feature_names())\n",
    "print(num_features)\n",
    "\n",
    "#Validation Data\n",
    "print('Testing with validation data:')\n",
    "val_pred = log_reg_clf.predict(X_val)\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Test Data\n",
    "print('Testing using test data:')\n",
    "test_pred = log_reg_clf.predict(X_test)\n",
    "report = classification_report(y_test, test_pred)\n",
    "print(report)\n",
    "print(\"------------------------------------------\")\n",
    "print(\"------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_df = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------TF-IDF--------------------\n",
      "TF-IDF Model with min_df=0.01\n",
      "3373\n",
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95      6361\n",
      "           1       0.95      0.96      0.95      6659\n",
      "\n",
      "    accuracy                           0.95     13020\n",
      "   macro avg       0.95      0.95      0.95     13020\n",
      "weighted avg       0.95      0.95      0.95     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95      6361\n",
      "           1       0.95      0.96      0.95      6660\n",
      "\n",
      "    accuracy                           0.95     13021\n",
      "   macro avg       0.95      0.95      0.95     13021\n",
      "weighted avg       0.95      0.95      0.95     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------TF-IDF--------------------')\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.01)\n",
    "tfidf_vectorizer.fit(X_train_text)\n",
    "\n",
    "X_train = tfidf_vectorizer.transform(X_train_text)\n",
    "X_val = tfidf_vectorizer.transform(X_val_text)\n",
    "X_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f'TF-IDF Model with min_df=0.01')\n",
    "log_reg_clf.fit(X_train, y_train)\n",
    "num_features = len(tfidf_vectorizer.get_feature_names())\n",
    "print(num_features)\n",
    "# tfidf_numfeatures.append(num_features)\n",
    "\n",
    "#Validation Data\n",
    "print('Testing with validation data:')\n",
    "val_pred = log_reg_clf.predict(X_val)\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Test Data\n",
    "print('Testing using test data:')\n",
    "test_pred = log_reg_clf.predict(X_test)\n",
    "report = classification_report(y_test, test_pred)\n",
    "print(report)\n",
    "print(\"------------------------------------------\")\n",
    "print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_df = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------TF-IDF--------------------\n",
      "TF-IDF Model with min_df=0.15\n",
      "134\n",
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      6361\n",
      "           1       0.90      0.92      0.91      6659\n",
      "\n",
      "    accuracy                           0.90     13020\n",
      "   macro avg       0.90      0.90      0.90     13020\n",
      "weighted avg       0.90      0.90      0.90     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      6361\n",
      "           1       0.90      0.92      0.91      6660\n",
      "\n",
      "    accuracy                           0.91     13021\n",
      "   macro avg       0.91      0.91      0.91     13021\n",
      "weighted avg       0.91      0.91      0.91     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------TF-IDF--------------------')\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.15)\n",
    "tfidf_vectorizer.fit(X_train_text)\n",
    "\n",
    "X_train = tfidf_vectorizer.transform(X_train_text)\n",
    "X_val = tfidf_vectorizer.transform(X_val_text)\n",
    "X_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f'TF-IDF Model with min_df=0.15')\n",
    "log_reg_clf.fit(X_train, y_train)\n",
    "num_features = len(tfidf_vectorizer.get_feature_names())\n",
    "print(num_features)\n",
    "# tfidf_numfeatures.append(num_features)\n",
    "\n",
    "#Validation Data\n",
    "print('Testing with validation data:')\n",
    "val_pred = log_reg_clf.predict(X_val)\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Test Data\n",
    "print('Testing using test data:')\n",
    "test_pred = log_reg_clf.predict(X_test)\n",
    "report = classification_report(y_test, test_pred)\n",
    "print(report)\n",
    "print(\"------------------------------------------\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# precision, recall, f1score = get_weighted_average(report)\n",
    "# tfidf_precision.append(precision)\n",
    "# tfidf_recall.append(recall)\n",
    "# tfidf_f1score.append(f1score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_features instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Model with min_df=0.2\n",
      "10000\n",
      "Testing with validation data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96      6361\n",
      "           1       0.96      0.96      0.96      6659\n",
      "\n",
      "    accuracy                           0.96     13020\n",
      "   macro avg       0.96      0.96      0.96     13020\n",
      "weighted avg       0.96      0.96      0.96     13020\n",
      "\n",
      "------------------------------------------\n",
      "Testing using test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95      6361\n",
      "           1       0.95      0.96      0.96      6660\n",
      "\n",
      "    accuracy                           0.96     13021\n",
      "   macro avg       0.96      0.96      0.96     13021\n",
      "weighted avg       0.96      0.96      0.96     13021\n",
      "\n",
      "------------------------------------------\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_features=10000) #, max_df=max_value\n",
    "tfidf_vectorizer.fit(X_train_text)\n",
    "\n",
    "X_train = tfidf_vectorizer.transform(X_train_text)\n",
    "X_val = tfidf_vectorizer.transform(X_val_text)\n",
    "X_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f'TF-IDF Model with min_df=0.2') #, max_df={max_value}\n",
    "log_reg_clf.fit(X_train, y_train)\n",
    "print(len(tfidf_vectorizer.get_feature_names()))\n",
    "\n",
    "#Validation Data\n",
    "print('Testing with validation data:')\n",
    "val_pred = log_reg_clf.predict(X_val)\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Test Data\n",
    "print('Testing using test data:')\n",
    "test_pred = log_reg_clf.predict(X_test)\n",
    "print(classification_report(y_test, test_pred))\n",
    "print(\"------------------------------------------\")\n",
    "print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding out which words were eliminated and kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '10', '10 percent', '10 year', '100', '1000', '10000', '100000', '11', '12', '13', '14', '15', '150', '16', '17', '18', '19', '1960', '1970', '1980', '1990', '20', '20 percent', '20 year', '200', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2016 elect', '2016 presidenti', '2017', '2018', '2019', '2020', '21', '21st', '21st centuri', '21wire', '21wiretv', '22', '23', '24', '25', '26', '27', '28', '29', '30', '300', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '400', '41', '42', '43', '44', '45', '46', '48', '49', '50', '500', '51', '52', '55', '60', '600', '65', '70', '75', '80', '800', '90', '911', 'abandon', 'abc', 'abil', 'abl', 'abort', 'abroad', 'absenc', 'absolut', 'absurd', 'abus', 'academ', 'academi', 'acceler', 'accept', 'access', 'accid', 'accompani', 'accomplish', 'accord', 'accord report', 'account', 'accur', 'accus', 'achiev', 'acknowledg', 'acquir', 'act', 'action', 'activ', 'activist', 'actor', 'actress', 'actual', 'ad', 'adam', 'add', 'addit', 'address', 'adjust', 'administr', 'administr offici', 'admir', 'admiss', 'admit', 'adopt', 'adult', 'advanc', 'advantag', 'advertis', 'advic', 'advis', 'advisor', 'advoc', 'advocaci', 'affair', 'affect', 'affili', 'afford', 'afford care', 'afghanistan', 'afraid', 'africa', 'african', 'africanamerican', 'aftermath', 'afternoon', 'afterward', 'age', 'agenc', 'agenc said', 'agenda', 'agent', 'aggress', 'ago', 'agre', 'agreement', 'agricultur', 'ahead', 'aid', 'aim', 'air', 'air forc', 'aircraft', 'airlin', 'airport', 'al', 'al qaeda', 'alabama', 'alarm', 'alert', 'alex', 'ali', 'alien', 'align', 'aliv', 'alleg', 'allegedli', 'alli', 'allianc', 'allow', 'alon', 'alongsid', 'alreadi', 'alter', 'altern', 'alway', 'amaz', 'ambassador', 'ambit', 'amend', 'america', 'america great', 'american', 'american citizen', 'american peopl', 'amid', 'analysi', 'analyst', 'anchor', 'andrew', 'angel', 'angela', 'angela merkel', 'anger', 'angri', 'anim', 'ann', 'announc', 'annual', 'anonym', 'anoth', 'answer', 'answer question', 'anthoni', 'anticip', 'antitrump', 'anybodi', 'anymor', 'anyon', 'anyth', 'anywher', 'apart', 'apolog', 'appar', 'appeal', 'appear', 'appl', 'appli', 'applic', 'appoint', 'appreci', 'approach', 'appropri', 'approv', 'approxim', 'april', 'arab', 'arabia', 'area', 'aren', 'arent', 'argu', 'argument', 'arizona', 'arm', 'armi', 'arrang', 'arrest', 'arriv', 'art', 'articl', 'artist', 'asia', 'asian', 'asid', 'ask', 'ask question', 'aspect', 'ass', 'assad', 'assassin', 'assault', 'assembl', 'assert', 'assess', 'asset', 'assign', 'assist', 'associ', 'associ press', 'assum', 'assur', 'asylum', 'athlet', 'atlant', 'atmospher', 'attach', 'attack', 'attempt', 'attend', 'attent', 'attitud', 'attorney', 'attorney gener', 'attract', 'attribut', 'audienc', 'audio', 'aug', 'august', 'australia', 'author', 'automat', 'avail', 'avenu', 'averag', 'avoid', 'await', 'awar', 'award', 'away', 'babi', 'background', 'backlash', 'bad', 'badli', 'bag', 'balanc', 'ball', 'ballist', 'ballist missil', 'ballot', 'ban', 'band', 'bank', 'banker', 'bannon', 'bar', 'barack', 'barack obama', 'bare', 'barrier', 'base', 'bashar', 'basi', 'basic', 'bathroom', 'battl', 'bay', 'beach', 'bear', 'beat', 'beauti', 'becam', 'becom', 'becom member', 'becom presid', 'bed', 'beg', 'began', 'begin', 'begun', 'behalf', 'behavior', 'beij', 'belief', 'believ', 'belong', 'ben', 'benefit', 'benghazi', 'benjamin', 'berlin', 'berni', 'berni sander', 'besid', 'best', 'bet', 'betray', 'better', 'bia', 'bid', 'big', 'bigger', 'biggest', 'bigot', 'bilater', 'billion', 'billion dollar', 'billionair', 'bipartisan', 'birth', 'bit', 'bitter', 'bizarr', 'black', 'black live', 'blame', 'blast', 'bless', 'blind', 'bloc', 'block', 'blog', 'blood', 'bloodi', 'blow', 'blue', 'board', 'boast', 'boat', 'bob', 'bodi', 'bolster', 'bomb', 'bond', 'book', 'boom', 'boost', 'border', 'born', 'boss', 'boston', 'bother', 'bought', 'bound', 'box', 'boy', 'brag', 'brain', 'branch', 'brand', 'brave', 'brazil', 'breach', 'break', 'breath', 'breitbart', 'breitbart news', 'brexit', 'brian', 'bridg', 'brief', 'briefli', 'brilliant', 'bring', 'britain', 'british', 'broad', 'broadcast', 'broader', 'broke', 'broken', 'brooklyn', 'brother', 'brought', 'brown', 'brussel', 'brutal', 'bu', 'budget', 'build', 'build wall', 'built', 'bullet', 'bulli', 'bunch', 'burden', 'bureau', 'buri', 'burn', 'bush', 'busi', 'businessman', 'bust', 'buy', 'cabinet', 'cabl', 'calcul', 'california', 'caller', 'calm', 'came', 'camera', 'camp', 'campaign', 'campaign manag', 'campaign promis', 'campaign trail', 'campu', 'canada', 'canadian', 'cancel', 'cancer', 'candid', 'candid donald', 'candid hillari', 'candidaci', 'cap', 'capabl', 'capac', 'capit', 'capitol', 'capitol hill', 'captur', 'car', 'card', 'care', 'care act', 'career', 'carolina', 'carri', 'carter', 'case', 'cash', 'cast', 'casualti', 'catastroph', 'catch', 'categori', 'cathol', 'caucu', 'caught', 'caus', 'caution', 'cb', 'celebr', 'cell', 'center', 'central', 'centuri', 'centuri wire', 'ceo', 'ceremoni', 'certain', 'certainli', 'chain', 'chair', 'chairman', 'challeng', 'chamber', 'champion', 'chanc', 'chancellor', 'chancellor angela', 'chang', 'channel', 'chant', 'chao', 'chapter', 'charact', 'character', 'charg', 'chariti', 'charl', 'chase', 'cheap', 'check', 'cheer', 'chemic', 'chicago', 'chief', 'chief execut', 'chief staff', 'child', 'children', 'china', 'chines', 'chip', 'choic', 'choos', 'chose', 'chosen', 'chri', 'christian', 'christma', 'chuck', 'church', 'cia', 'circl', 'circuit', 'circul', 'circumst', 'cite', 'citi', 'citizen', 'citizenship', 'civil', 'civil right', 'civil war', 'civilian', 'claim', 'clarifi', 'clash', 'class', 'classic', 'classifi', 'classifi inform', 'clean', 'clear', 'clearli', 'cleveland', 'click', 'client', 'climat', 'climat chang', 'climb', 'clinic', 'clinton', 'clinton campaign', 'clinton email', 'clinton foundat', 'clinton said', 'clip', 'close', 'closer', 'closest', 'cloth', 'club', 'cnn', 'coach', 'coal', 'coalit', 'coast', 'code', 'coincid', 'cold', 'cold war', 'collabor', 'collaps', 'colleagu', 'collect', 'colleg', 'collus', 'color', 'colorado', 'columbia', 'columnist', 'combat', 'combin', 'come', 'comey', 'comfort', 'command', 'comment', 'commentari', 'commerc', 'commerci', 'commiss', 'commission', 'commit', 'committe', 'committe chairman', 'committe said', 'common', 'commun', 'communist', 'compani', 'compar', 'comparison', 'compel', 'compet', 'competit', 'complain', 'complaint', 'complet', 'complex', 'compli', 'complic', 'compound', 'comprehens', 'compromis', 'comput', 'conced', 'concentr', 'concept', 'concern', 'concert', 'conclud', 'conclus', 'concret', 'condemn', 'condit', 'condit anonym', 'conduct', 'confer', 'confid', 'confirm', 'conflict', 'confront', 'confus', 'congress', 'congression', 'congressman', 'connect', 'consensu', 'consequ', 'conserv', 'consid', 'consider', 'consist', 'conspiraci', 'constant', 'constantli', 'constitu', 'constitut', 'construct', 'consult', 'consum', 'contact', 'contain', 'contend', 'content', 'contest', 'context', 'continu', 'contract', 'contractor', 'contradict', 'contrari', 'contrast', 'contribut', 'contributor', 'control', 'controversi', 'conveni', 'convent', 'convers', 'convict', 'convinc', 'cook', 'cool', 'cooper', 'coordin', 'cop', 'copi', 'core', 'corner', 'corp', 'corpor', 'correct', 'correspond', 'corrupt', 'cost', 'couldn', 'council', 'counsel', 'count', 'counter', 'counterpart', 'counti', 'countri', 'coup', 'coupl', 'courag', 'cours', 'court', 'court justic', 'court rule', 'cover', 'coverag', 'crack', 'crackdown', 'crash', 'crazi', 'creat', 'creation', 'creativ', 'credibl', 'credit', 'crew', 'cri', 'crime', 'crimin', 'crisi', 'critic', 'critic trump', 'crook', 'cross', 'crowd', 'crucial', 'crush', 'cruz', 'cuba', 'cultur', 'curb', 'currenc', 'current', 'custodi', 'custom', 'cut', 'cyber', 'cycl', 'daili', 'daili mail', 'dakota', 'damag', 'damn', 'dan', 'danc', 'danger', 'daniel', 'dare', 'dark', 'data', 'date', 'daughter', 'davi', 'david', 'day', 'dc', 'dead', 'deadli', 'deadlin', 'deal', 'death', 'debat', 'debt', 'dec', 'decad', 'decad ago', 'decemb', 'decid', 'decis', 'declar', 'declin', 'declin comment', 'dedic', 'deem', 'deep', 'deeper', 'deepli', 'defeat', 'defend', 'defens', 'defi', 'deficit', 'defin', 'definit', 'degre', 'delay', 'deleg', 'delet', 'deliber', 'deliv', 'dem', 'demand', 'democraci', 'democrat', 'democrat leader', 'democrat nation', 'democrat parti', 'democrat presid', 'democrat presidenti', 'democrat republican', 'democrat senat', 'demonstr', 'deni', 'denounc', 'depart', 'depart homeland', 'depart justic', 'depart said', 'departur', 'depend', 'depict', 'deploy', 'deport', 'depress', 'deputi', 'describ', 'descript', 'deserv', 'design', 'desir', 'desper', 'despit', 'destroy', 'destruct', 'detain', 'detect', 'detent', 'determin', 'detroit', 'devast', 'develop', 'devic', 'devot', 'dialogu', 'dictat', 'didn', 'didnt', 'die', 'differ', 'difficult', 'difficulti', 'dig', 'digit', 'dinner', 'diplomaci', 'diplomat', 'direct', 'directli', 'director', 'director jame', 'dirti', 'disabl', 'disagre', 'disappear', 'disappoint', 'disast', 'disclos', 'disclosur', 'discov', 'discredit', 'discrimin', 'discuss', 'diseas', 'disgrac', 'disgust', 'dismantl', 'dismiss', 'displac', 'display', 'disput', 'disrespect', 'disrupt', 'dissent', 'distanc', 'distinct', 'distract', 'distribut', 'district', 'disturb', 'divers', 'divid', 'divis', 'divorc', 'dnc', 'doctor', 'document', 'documentari', 'doesn', 'doesnt', 'dog', 'dollar', 'domest', 'domin', 'don', 'don know', 'don think', 'don want', 'donald', 'donald trump', 'donat', 'donor', 'dont', 'dont know', 'dont think', 'dont want', 'door', 'doubl', 'doubt', 'dozen', 'dr', 'draft', 'drag', 'drama', 'dramat', 'draw', 'drawn', 'dream', 'dress', 'drew', 'drill', 'drink', 'drive', 'driven', 'driver', 'drone', 'drop', 'drove', 'drug', 'dump', 'duti', 'eager', 'ear', 'earli', 'earlier', 'earlier month', 'earlier week', 'earlier year', 'earn', 'earth', 'eas', 'easi', 'easier', 'easili', 'east', 'eastern', 'eat', 'echo', 'econom', 'econom growth', 'economi', 'economist', 'edg', 'edit', 'editor', 'editori', 'educ', 'edward', 'effect', 'effici', 'effort', 'egypt', 'el', 'elabor', 'elect', 'elect campaign', 'elect day', 'elect offici', 'elect presid', 'elect trump', 'elector', 'electr', 'electron', 'element', 'elev', 'elig', 'elimin', 'elit', 'elizabeth', 'elsewher', 'email', 'email server', 'embarrass', 'embassi', 'embrac', 'emerg', 'emot', 'emphas', 'empir', 'employ', 'employe', 'empti', 'enabl', 'enact', 'encount', 'encourag', 'end', 'endors', 'endur', 'enemi', 'energi', 'enforc', 'engag', 'engin', 'england', 'english', 'enhanc', 'enjoy', 'enorm', 'ensur', 'enter', 'enterpris', 'entertain', 'entir', 'entiti', 'entitl', 'entri', 'environ', 'environment', 'episod', 'equal', 'equip', 'equival', 'era', 'eric', 'error', 'erupt', 'escal', 'escap', 'especi', 'essenti', 'establish', 'estat', 'estim', 'ethic', 'ethnic', 'eu', 'euro', 'europ', 'european', 'european union', 'evalu', 'event', 'eventu', 'everi', 'everi day', 'everi singl', 'everi time', 'everi year', 'everybodi', 'everyon', 'everyth', 'everywher', 'evid', 'evil', 'exact', 'exactli', 'examin', 'exampl', 'excel', 'excess', 'exchang', 'excit', 'exclud', 'exclus', 'excus', 'execut', 'execut director', 'execut order', 'exempt', 'exercis', 'exist', 'exit', 'expand', 'expans', 'expect', 'expens', 'experi', 'experienc', 'expert', 'expir', 'explain', 'explan', 'explod', 'exploit', 'explor', 'explos', 'export', 'expos', 'express', 'express concern', 'extend', 'extens', 'extent', 'extra', 'extraordinari', 'extrem', 'extremist', 'eye', 'fabric', 'face', 'facebook', 'facebook page', 'facil', 'fact', 'faction', 'factor', 'factori', 'fail', 'failur', 'fair', 'fairli', 'faith', 'fake', 'fake news', 'fall', 'fallen', 'fals', 'famili', 'famili member', 'familiar', 'famou', 'fan', 'far', 'farm', 'fashion', 'fast', 'fatal', 'fate', 'father', 'fault', 'favor', 'favorit', 'fbi', 'fbi director', 'fear', 'featur', 'featur imag', 'feb', 'februari', 'fed', 'feder', 'feder court', 'feder govern', 'feder judg', 'feder law', 'fee', 'feed', 'feel', 'feet', 'fell', 'fellow', 'felt', 'femal', 'fewer', 'field', 'fierc', 'fifth', 'fight', 'fighter', 'figur', 'file', 'film', 'final', 'financ', 'financi', 'fine', 'finger', 'finish', 'firearm', 'firm', 'fiscal', 'fish', 'fit', 'fix', 'flag', 'flaw', 'fled', 'flee', 'fli', 'flight', 'flip', 'flood', 'floor', 'florida', 'flow', 'flynn', 'focu', 'focus', 'folk', 'follow', 'follow twitter', 'food', 'foot', 'footag', 'footbal', 'forc', 'foreign', 'foreign minist', 'foreign ministri', 'foreign polici', 'forev', 'forget', 'form', 'formal', 'forth', 'fortun', 'forum', 'forward', 'fought', 'foundat', 'founder', 'fourth', 'fox', 'fox news', 'frame', 'franc', 'francisco', 'frank', 'frankli', 'fraud', 'free', 'free speech', 'free trade', 'freedom', 'french', 'frequent', 'fresh', 'friday', 'friend', 'friendli', 'frontrunn', 'frustrat', 'fuel', 'fulfil', 'fulli', 'fun', 'function', 'fund', 'fundament', 'fundrais', 'funni', 'futur', 'ga', 'gain', 'game', 'gang', 'gap', 'gari', 'gate', 'gather', 'gave', 'gay', 'gear', 'gender', 'gener', 'gener elect', 'genuin', 'georg', 'georg bush', 'georgia', 'german', 'germani', 'getti', 'getti imag', 'giant', 'gift', 'girl', 'given', 'glass', 'global', 'globe', 'goal', 'god', 'goe', 'gold', 'golden', 'golf', 'gon', 'gon na', 'gone', 'good', 'googl', 'gop', 'got', 'gotten', 'gov', 'govern', 'govern offici', 'govern said', 'governor', 'grab', 'graduat', 'graham', 'grand', 'grant', 'graphic', 'grave', 'great', 'greater', 'greatest', 'green', 'greet', 'grew', 'gross', 'ground', 'group', 'group said', 'grow', 'grown', 'growth', 'guarante', 'guard', 'guardian', 'guess', 'guest', 'guid', 'guilti', 'gulf', 'gun', 'guy', 'hack', 'hacker', 'hail', 'hair', 'half', 'hall', 'halt', 'hammer', 'hampshir', 'hand', 'handl', 'hang', 'happen', 'happi', 'harass', 'hard', 'harder', 'hardli', 'harm', 'harri', 'harsh', 'harvard', 'hasn', 'hat', 'hate', 'hatr', 'haven', 'havent', 'head', 'headlin', 'headquart', 'health', 'health care', 'health insur', 'healthcar', 'healthi', 'hear', 'heard', 'heart', 'heat', 'heavi', 'heavili', 'hedg', 'height', 'held', 'hell', 'help', 'heritag', 'hero', 'hey', 'hidden', 'hide', 'high', 'high school', 'higher', 'highest', 'highli', 'highlight', 'hilari', 'hill', 'hillari', 'hillari clinton', 'hint', 'hire', 'hispan', 'histor', 'histori', 'hit', 'hold', 'holder', 'hole', 'holi', 'holiday', 'hollywood', 'home', 'homeland', 'homeland secur', 'honest', 'honor', 'hope', 'horribl', 'horrif', 'hospit', 'host', 'hostil', 'hot', 'hotel', 'hour', 'hous', 'hous offici', 'hous press', 'hous repres', 'hous republican', 'hous said', 'hous senat', 'hous speaker', 'household', 'huge', 'human', 'human right', 'humanitarian', 'humili', 'hundr', 'hundr thousand', 'hunt', 'hurrican', 'hurt', 'husband', 'ice', 'id', 'idea', 'ideal', 'ident', 'identifi', 'ideolog', 'idiot', 'ignor', 'ii', 'ill', 'illeg', 'illeg alien', 'illeg immigr', 'illinoi', 'illustr', 'im', 'imag', 'imag video', 'imagin', 'immedi', 'immigr', 'immun', 'impact', 'impeach', 'implement', 'impli', 'implic', 'import', 'impos', 'imposs', 'impress', 'improv', 'inappropri', 'inaugur', 'incid', 'incit', 'includ', 'inclus', 'incom', 'increas', 'increasingli', 'incred', 'inde', 'independ', 'india', 'indian', 'indiana', 'indic', 'indict', 'individu', 'industri', 'inequ', 'inevit', 'infam', 'inflat', 'influenc', 'influenti', 'inform', 'infrastructur', 'initi', 'injur', 'injuri', 'inner', 'innoc', 'innov', 'inquiri', 'insan', 'insid', 'insist', 'inspir', 'instal', 'instanc', 'instead', 'institut', 'instruct', 'insult', 'insur', 'insurg', 'integr', 'intellectu', 'intellig', 'intellig agenc', 'intellig committe', 'intend', 'intens', 'intent', 'interact', 'interfer', 'interior', 'intern', 'internet', 'interpret', 'interven', 'intervent', 'interview', 'intimid', 'introduc', 'invad', 'invas', 'invest', 'investig', 'investor', 'invit', 'involv', 'iowa', 'iran', 'iranian', 'iraq', 'iraqi', 'iron', 'isi', 'islam', 'islam state', 'islamist', 'island', 'isn', 'isnt', 'isol', 'isra', 'israel', 'issu', 'itali', 'italian', 'item', 'ivanka', 'ive', 'jack', 'jail', 'jame', 'jame comey', 'jan', 'jan 20', 'januari', 'japan', 'japanes', 'jare', 'jason', 'jay', 'jeff', 'jeff session', 'jersey', 'jerusalem', 'jet', 'jew', 'jewish', 'jihadist', 'jim', 'jimmi', 'jinp', 'job', 'joe', 'john', 'john mccain', 'john podesta', 'johnson', 'join', 'joint', 'joke', 'jone', 'jordan', 'joseph', 'josh', 'journal', 'journalist', 'journey', 'joy', 'jr', 'judg', 'judgment', 'judici', 'judiciari', 'judiciari committe', 'juli', 'jump', 'june', 'juri', 'justic', 'justic depart', 'justifi', 'justin', 'kelli', 'kennedi', 'kentucki', 'kept', 'kerri', 'kevin', 'key', 'kick', 'kid', 'kill', 'killer', 'kim', 'kind', 'king', 'kingdom', 'km', 'knew', 'knock', 'know', 'knowledg', 'known', 'korea', 'korean', 'kremlin', 'kurdish', 'la', 'label', 'labor', 'lack', 'ladi', 'laid', 'land', 'languag', 'larg', 'larger', 'largest', 'lash', 'late', 'later', 'latest', 'latin', 'latino', 'laugh', 'launch', 'law', 'law enforc', 'lawmak', 'lawsuit', 'lawyer', 'lay', 'lead', 'leader', 'leader mitch', 'leadership', 'leagu', 'leak', 'lean', 'learn', 'leav', 'led', 'lee', 'left', 'leftist', 'leg', 'legaci', 'legal', 'legisl', 'legislatur', 'legitim', 'lesson', 'let', 'letter', 'level', 'liar', 'liber', 'liberti', 'libya', 'licens', 'lie', 'life', 'lifetim', 'lift', 'light', 'like', 'limit', 'line', 'link', 'list', 'listen', 'liter', 'littl', 'littl bit', 'live', 'live matter', 'll', 'lo', 'lo angel', 'load', 'loan', 'lobbi', 'lobbyist', 'local', 'locat', 'lock', 'logic', 'london', 'london reuter', 'long', 'long time', 'longer', 'longstand', 'longterm', 'longtim', 'look', 'look forward', 'lose', 'loss', 'lost', 'lot', 'lot peopl', 'love', 'low', 'lower', 'loyal', 'lynch', 'machin', 'mad', 'magazin', 'mail', 'main', 'mainli', 'mainstream', 'mainstream media', 'maintain', 'major', 'major leader', 'make', 'make america', 'make sens', 'make sure', 'maker', 'male', 'man', 'manag', 'mandat', 'manhattan', 'mani', 'mani peopl', 'mani year', 'manipul', 'manner', 'manufactur', 'map', 'march', 'marco', 'marco rubio', 'margin', 'mari', 'marin', 'mark', 'market', 'marri', 'marriag', 'martin', 'mass', 'massachusett', 'massacr', 'massiv', 'master', 'match', 'mate', 'materi', 'matt', 'matter', 'matthew', 'maximum', 'mayb', 'mayor', 'mccain', 'mcconnel', 'mean', 'meant', 'meanwhil', 'measur', 'mechan', 'meddl', 'media', 'media outlet', 'media report', 'medic', 'medicin', 'meet', 'member', 'member 21wiretv', 'member congress', 'membership', 'memo', 'memori', 'men', 'men women', 'mental', 'mention', 'mere', 'merkel', 'mess', 'messag', 'met', 'method', 'mexican', 'mexico', 'michael', 'michael flynn', 'michel', 'michigan', 'middl', 'middl east', 'migrant', 'migrat', 'mike', 'mike penc', 'mile', 'milit', 'militari', 'militia', 'miller', 'million', 'million american', 'million dollar', 'million peopl', 'mind', 'minimum', 'minist', 'minist theresa', 'ministri', 'ministri said', 'minnesota', 'minor', 'minut', 'mirror', 'miss', 'missil', 'mission', 'missouri', 'mistak', 'mitch', 'mitch mcconnel', 'mitt', 'mitt romney', 'mix', 'mobil', 'mock', 'model', 'moder', 'modern', 'mogul', 'moham', 'mom', 'moment', 'monday', 'money', 'monitor', 'month', 'month ago', 'moor', 'moral', 'morn', 'moscow', 'mosqu', 'mostli', 'mother', 'motion', 'motiv', 'mount', 'mountain', 'mouth', 'movement', 'movi', 'mr', 'mr clinton', 'mr obama', 'mr trump', 'ms', 'msnbc', 'multipl', 'murder', 'music', 'muslim', 'mutual', 'mysteri', 'na', 'nanci', 'narr', 'narrow', 'nation', 'nation committe', 'nation convent', 'nation secur', 'nationalist', 'nationwid', 'nativ', 'nato', 'natur', 'navi', 'nazi', 'nbc', 'near', 'nearbi', 'nearli', 'necessari', 'necessarili', 'need', 'neg', 'negoti', 'neighbor', 'neighborhood', 'net', 'network', 'neutral', 'nevada', 'new', 'new hampshir', 'new jersey', 'new presid', 'new york', 'newli', 'news', 'news 21st', 'news agenc', 'news confer', 'news media', 'news report', 'newspap', 'nice', 'night', 'nobodi', 'nomin', 'nomine', 'nomine donald', 'nonprofit', 'normal', 'north', 'north carolina', 'north korea', 'north korean', 'northern', 'notabl', 'note', 'noth', 'notic', 'notion', 'notori', 'nov', 'nov elect', 'novemb', 'novemb 2016', 'nuclear', 'nuclear weapon', 'number', 'numer', 'obama', 'obama administr', 'obama said', 'obamacar', 'object', 'oblig', 'observ', 'obsess', 'obstruct', 'obtain', 'obviou', 'obvious', 'occas', 'occasion', 'occup', 'occupi', 'occur', 'oct', 'octob', 'odd', 'offend', 'offens', 'offer', 'offic', 'offic said', 'offici', 'offici said', 'offici say', 'offici told', 'oh', 'ohio', 'oil', 'ok', 'okay', 'old', 'older', 'ongo', 'onlin', 'open', 'openli', 'oper', 'opinion', 'opinion poll', 'oppon', 'opportun', 'oppos', 'opposit', 'oppress', 'option', 'orang', 'order', 'ordinari', 'oregon', 'organ', 'origin', 'orlando', 'oust', 'outcom', 'outlet', 'outlin', 'outrag', 'outright', 'outsid', 'oval', 'oval offic', 'overal', 'overcom', 'overhaul', 'overse', 'oversea', 'oversight', 'overturn', 'overwhelm', 'overwhelmingli', 'owe', 'owner', 'pace', 'pacif', 'pack', 'packag', 'pact', 'page', 'paid', 'pain', 'paint', 'pair', 'pakistan', 'palestinian', 'panel', 'paper', 'paragraph', 'parent', 'pari', 'park', 'parliament', 'parliamentari', 'parti', 'parti leader', 'partial', 'particip', 'particular', 'particularli', 'partisan', 'partli', 'partner', 'partnership', 'pass', 'passag', 'passeng', 'passion', 'past', 'past year', 'path', 'patient', 'patrick', 'patriot', 'patrol', 'pattern', 'paul', 'paul ryan', 'pay', 'payment', 'peac', 'pen', 'penalti', 'penc', 'pend', 'peninsula', 'pennsylvania', 'pentagon', 'peopl', 'peopl kill', 'peopl live', 'peopl said', 'peopl vote', 'peopl want', 'perceiv', 'percent', 'percentag', 'percept', 'perfect', 'perfectli', 'perform', 'perhap', 'period', 'perman', 'permiss', 'permit', 'persecut', 'persist', 'person', 'personnel', 'perspect', 'persuad', 'peter', 'petit', 'phase', 'philadelphia', 'phone', 'photo', 'photograph', 'phrase', 'physic', 'pick', 'pictur', 'piec', 'pipelin', 'place', 'plan', 'plane', 'planet', 'plant', 'platform', 'play', 'player', 'plead', 'pleas', 'pledg', 'plenti', 'plot', 'plu', 'pm', 'pocket', 'podesta', 'point', 'poison', 'polic', 'polic depart', 'polic offic', 'polic said', 'polici', 'polit', 'polit parti', 'politician', 'politico', 'poll', 'pool', 'poor', 'pop', 'popul', 'popular', 'populist', 'port', 'portion', 'portray', 'pose', 'posit', 'possess', 'possibl', 'post', 'potenti', 'potu', 'pound', 'pour', 'poverti', 'power', 'practic', 'prais', 'pray', 'prayer', 'preced', 'precis', 'predecessor', 'predict', 'prefer', 'premium', 'prepar', 'presenc', 'present', 'preserv', 'presid', 'presid barack', 'presid bashar', 'presid clinton', 'presid donald', 'presid georg', 'presid obama', 'presid said', 'presid trump', 'presid unit', 'presid vladimir', 'presidentelect', 'presidentelect donald', 'presidenti', 'presidenti campaign', 'presidenti candid', 'presidenti elect', 'presidenti nomine', 'presidenti race', 'press', 'press confer', 'press secretari', 'pressur', 'presum', 'presumpt', 'pretend', 'pretti', 'prevent', 'previou', 'previous', 'price', 'pride', 'primari', 'primarili', 'prime', 'prime minist', 'princip', 'principl', 'print', 'prior', 'prioriti', 'prison', 'privaci', 'privat', 'privat email', 'privileg', 'prize', 'probabl', 'probe', 'problem', 'procedur', 'proceed', 'process', 'produc', 'product', 'profession', 'professor', 'profil', 'profit', 'program', 'progress', 'prohibit', 'project', 'promin', 'promis', 'promot', 'prompt', 'proof', 'propaganda', 'proper', 'properli', 'properti', 'propos', 'prosecut', 'prosecutor', 'prospect', 'prosper', 'protect', 'protest', 'proud', 'prove', 'proven', 'provid', 'provinc', 'provis', 'provoc', 'provok', 'psycholog', 'public', 'publicli', 'publish', 'pull', 'punch', 'pundit', 'punish', 'purchas', 'pure', 'purpos', 'pursu', 'push', 'putin', 'pyongyang', 'qaeda', 'qualifi', 'qualiti', 'quarter', 'queen', 'question', 'quick', 'quickli', 'quiet', 'quietli', 'quit', 'quot', 'race', 'racial', 'racism', 'racist', 'radic', 'radio', 'rage', 'raid', 'rail', 'rain', 'rais', 'rais question', 'ralli', 'ran', 'rang', 'rank', 'rant', 'rape', 'rapid', 'rapidli', 'rapist', 'rare', 'rate', 'reach', 'react', 'reaction', 'read', 'reader', 'readi', 'reagan', 'real', 'real estat', 'realdonaldtrump', 'realiti', 'realiz', 'realli', 'reason', 'reassur', 'rebel', 'rebuild', 'recal', 'receiv', 'recent', 'recent month', 'recent week', 'recent year', 'recess', 'recogn', 'recognit', 'recommend', 'record', 'recount', 'recov', 'recruit', 'red', 'reduc', 'reduct', 'reelect', 'refer', 'referendum', 'reflect', 'reform', 'refuge', 'refus', 'regard', 'regardless', 'regim', 'region', 'regist', 'regret', 'regul', 'regular', 'regularli', 'regulatori', 'reinforc', 'reiter', 'reject', 'rel', 'relat', 'relationship', 'releas', 'relev', 'reli', 'reliabl', 'relief', 'religi', 'religion', 'reluct', 'remain', 'remark', 'rememb', 'remind', 'remot', 'remov', 'renew', 'rent', 'reopen', 'rep', 'repeal', 'repeat', 'repeatedli', 'replac', 'repli', 'report', 'report said', 'reportedli', 'repres', 'republ', 'republican', 'republican candid', 'republican democrat', 'republican lawmak', 'republican leader', 'republican nation', 'republican nomine', 'republican parti', 'republican presid', 'republican presidenti', 'republican senat', 'reput', 'request', 'request comment', 'requir', 'rescu', 'research', 'reserv', 'resid', 'resign', 'resist', 'resolut', 'resolv', 'resort', 'resourc', 'respect', 'respond', 'respond request', 'respons', 'rest', 'restaur', 'restor', 'restrict', 'result', 'resum', 'retain', 'retali', 'retir', 'retreat', 'return', 'reuter', 'reuter presid', 'reuter republican', 'reveal', 'revel', 'revenu', 'revers', 'review', 'revis', 'reviv', 'revolut', 'revolutionari', 'reward', 'rex', 'rex tillerson', 'rhetor', 'rich', 'richard', 'rick', 'rid', 'ride', 'ridicul', 'rifl', 'rig', 'right', 'right group', 'rightw', 'ring', 'riot', 'rip', 'rise', 'risk', 'rival', 'river', 'road', 'rob', 'robert', 'rock', 'roger', 'role', 'roll', 'romney', 'ron', 'ronald', 'room', 'root', 'rose', 'roughli', 'round', 'rout', 'routin', 'row', 'royal', 'rubio', 'ruin', 'rule', 'rule law', 'rumor', 'run', 'run presid', 'rural', 'rush', 'russia', 'russian', 'russian govern', 'russian presid', 'ryan', 'sad', 'safe', 'safeti', 'said', 'said ad', 'said believ', 'said email', 'said friday', 'said hope', 'said interview', 'said mani', 'said monday', 'said mr', 'said peopl', 'said plan', 'said presid', 'said refer', 'said said', 'said statement', 'said sunday', 'said think', 'said thursday', 'said time', 'said trump', 'said tuesday', 'said twitter', 'said want', 'said wednesday', 'said week', 'salari', 'sale', 'san', 'san francisco', 'sanction', 'sander', 'sarah', 'sat', 'satisfi', 'saturday', 'saudi', 'saudi arabia', 'save', 'saw', 'say', 'say trump', 'say want', 'scale', 'scandal', 'scare', 'scenario', 'scene', 'schedul', 'scheme', 'school', 'schumer', 'scienc', 'scientif', 'scientist', 'scope', 'score', 'scott', 'scrap', 'scream', 'screen', 'screen captur', 'screenshot', 'script', 'scrutini', 'sea', 'seal', 'sean', 'sean spicer', 'search', 'season', 'seat', 'second', 'secret', 'secretari', 'secretari state', 'section', 'sector', 'secur', 'secur advis', 'secur council', 'secur forc', 'seek', 'seemingli', 'seen', 'segment', 'seiz', 'select', 'sell', 'sen', 'senat', 'senat democrat', 'senat john', 'senat major', 'senat republican', 'send', 'senior', 'senior offici', 'sens', 'sensit', 'sent', 'sentenc', 'sentiment', 'separ', 'sept', 'septemb', 'seri', 'seriou', 'serv', 'server', 'servic', 'session', 'set', 'settl', 'settlement', 'seven', 'sever', 'sever year', 'sex', 'sexual', 'sexual assault', 'shadow', 'shake', 'shame', 'shape', 'share', 'sharp', 'sharpli', 'shed', 'shelter', 'sheriff', 'shi', 'shield', 'shift', 'ship', 'shock', 'shoot', 'shop', 'short', 'shortli', 'shot', 'shouldn', 'shouldnt', 'shout', 'shown', 'shut', 'sick', 'sight', 'sign', 'signal', 'signatur', 'signific', 'significantli', 'silenc', 'silent', 'silver', 'similar', 'simpl', 'simpli', 'sing', 'singer', 'singl', 'sink', 'sister', 'sit', 'site', 'situat', 'size', 'skeptic', 'skill', 'skin', 'sky', 'slam', 'slash', 'sleep', 'slightli', 'slip', 'slogan', 'slow', 'slowli', 'small', 'smaller', 'smart', 'smile', 'smith', 'smoke', 'socal', 'social', 'social media', 'socialist', 'societi', 'sold', 'soldier', 'sole', 'solid', 'solut', 'solv', 'somebodi', 'someon', 'someth', 'sometim', 'somewhat', 'son', 'song', 'soon', 'soro', 'sorri', 'sort', 'sought', 'soul', 'sound', 'sourc', 'sourc said', 'south', 'south carolina', 'south korea', 'southeast', 'southern', 'sovereignti', 'soviet', 'space', 'spain', 'spanish', 'spark', 'speak', 'speaker', 'speaker paul', 'special', 'special counsel', 'specif', 'specul', 'speech', 'speed', 'spend', 'spent', 'spi', 'spicer', 'spin', 'spirit', 'split', 'spoke', 'spoken', 'spokesman', 'spokesman said', 'spokesperson', 'spokeswoman', 'sponsor', 'sport', 'spot', 'spread', 'spring', 'squar', 'st', 'stabil', 'stabl', 'staff', 'staffer', 'stage', 'stake', 'stall', 'stanc', 'stand', 'standard', 'star', 'start', 'state', 'state depart', 'state govern', 'state rex', 'state said', 'state senat', 'state trump', 'statement', 'statement said', 'station', 'statist', 'statu', 'stay', 'steal', 'stem', 'step', 'stephen', 'steve', 'steven', 'stick', 'stir', 'stock', 'stolen', 'stone', 'stood', 'stop', 'store', 'stori', 'storm', 'straight', 'strain', 'strang', 'strateg', 'strategi', 'strategist', 'stream', 'street', 'street journal', 'strength', 'strengthen', 'stress', 'stretch', 'strike', 'string', 'strip', 'strong', 'stronger', 'strongli', 'struck', 'structur', 'struggl', 'stuck', 'student', 'studi', 'stuff', 'stun', 'stupid', 'style', 'su', 'subject', 'submit', 'subscrib', 'subscrib becom', 'subsequ', 'subsidi', 'substanti', 'succeed', 'success', 'successor', 'sudden', 'suddenli', 'sue', 'suffer', 'suffici', 'suggest', 'suicid', 'suit', 'sum', 'summer', 'summit', 'sun', 'sunday', 'sunni', 'super', 'suppli', 'support', 'support trump', 'suppos', 'supposedli', 'suppress', 'suprem', 'suprem court', 'supremacist', 'sure', 'surfac', 'surg', 'surpris', 'surround', 'surveil', 'survey', 'surviv', 'susan', 'suspect', 'suspend', 'suspici', 'suspicion', 'sustain', 'sweep', 'swing', 'switch', 'sworn', 'symbol', 'syria', 'syrian', 'tabl', 'tackl', 'tactic', 'tag', 'taken', 'talent', 'talk', 'tank', 'tap', 'tape', 'target', 'task', 'taught', 'tax', 'tax cut', 'taxpay', 'tea', 'teach', 'teacher', 'team', 'tear', 'tech', 'technic', 'technolog', 'ted', 'ted cruz', 'teenag', 'tehran', 'telephon', 'televis', 'tell', 'temporari', 'temporarili', 'tend', 'tension', 'tenur', 'term', 'termin', 'terribl', 'terrifi', 'territori', 'terror', 'terrorist', 'terrorist attack', 'test', 'testifi', 'testimoni', 'texa', 'text', 'thank', 'theater', 'theme', 'theori', 'theresa', 'theyr', 'theyv', 'thing', 'think', 'think tank', 'thoma', 'thought', 'thousand', 'thousand peopl', 'threat', 'threaten', 'threw', 'throw', 'thrown', 'thu', 'thursday', 'ticket', 'tie', 'tight', 'tighten', 'tillerson', 'tim', 'time', 'time report', 'timeswashington', 'tini', 'tip', 'tire', 'titl', 'today', 'togeth', 'told', 'told news', 'told report', 'told reuter', 'toler', 'toll', 'tom', 'tomorrow', 'tone', 'tonight', 'took', 'took offic', 'took place', 'tool', 'topic', 'tortur', 'total', 'touch', 'tough', 'tougher', 'tour', 'tourist', 'tower', 'town', 'track', 'trade', 'trade agreement', 'trade deal', 'tradit', 'traffic', 'tragedi', 'trail', 'train', 'transcript', 'transfer', 'transform', 'transgend', 'transit', 'transit team', 'translat', 'transpar', 'transport', 'trap', 'trash', 'travel', 'treasuri', 'treat', 'treati', 'treatment', 'tree', 'tremend', 'trend', 'tri', 'tri make', 'trial', 'trigger', 'trillion', 'trip', 'troop', 'troubl', 'truck', 'true', 'truli', 'trump', 'trump administr', 'trump campaign', 'trump elect', 'trump make', 'trump new', 'trump plan', 'trump presid', 'trump promis', 'trump ralli', 'trump realdonaldtrump', 'trump republican', 'trump said', 'trump say', 'trump support', 'trump told', 'trump took', 'trump tower', 'trump tweet', 'trump victori', 'trump want', 'trump white', 'trump win', 'trump won', 'trust', 'truth', 'tuesday', 'tune', 'turkey', 'turkish', 'turn', 'tv', 'tweet', 'twice', 'twist', 'twitter', 'twitter account', 'type', 'typic', 'ugli', 'uk', 'ukrain', 'ultim', 'unabl', 'unaccept', 'unanim', 'uncertainti', 'unclear', 'unconstitut', 'uncov', 'undermin', 'underscor', 'understand', 'understood', 'undocu', 'unemploy', 'unfair', 'unfold', 'unfortun', 'uniform', 'union', 'uniqu', 'unit', 'unit nation', 'unit state', 'uniti', 'univers', 'unknown', 'unleash', 'unless', 'unlik', 'unpreced', 'unusu', 'unveil', 'upcom', 'updat', 'upper', 'upset', 'urban', 'urg', 'urgent', 'usa', 'use', 'use privat', 'user', 'usual', 'util', 'vacat', 'valid', 'valley', 'valu', 'van', 'varieti', 'variou', 'vast', 've', 'vehicl', 'ventur', 'verifi', 'vermont', 'version', 'vet', 'veteran', 'veto', 'vice', 'vice presid', 'victim', 'victori', 'video', 'video screen', 'vietnam', 'view', 'viewer', 'villag', 'violat', 'violenc', 'violent', 'viral', 'virginia', 'virtual', 'visa', 'visibl', 'vision', 'visit', 'visitor', 'vital', 'vladimir', 'vladimir putin', 'vocal', 'voic', 'volunt', 'vote', 'vote trump', 'voter', 'vow', 'vulner', 'wage', 'wait', 'wake', 'walk', 'wall', 'wall street', 'want', 'want know', 'want make', 'war', 'warm', 'warn', 'warrant', 'warren', 'washington', 'washington dc', 'washington post', 'washington reuter', 'wasn', 'wasnt', 'wast', 'watch', 'watchdog', 'water', 'wave', 'way', 'weak', 'weaken', 'wealth', 'wealthi', 'weapon', 'wear', 'weather', 'web', 'websit', 'wed', 'wednesday', 'week', 'week ago', 'weekend', 'weekli', 'weigh', 'weight', 'welcom', 'welfar', 'went', 'west', 'western', 'weve', 'whatev', 'whenev', 'whine', 'white', 'white hous', 'white supremacist', 'wide', 'widespread', 'wife', 'wikileak', 'wild', 'william', 'win', 'win elect', 'wind', 'window', 'wing', 'winner', 'winter', 'wire', 'wire say', 'wisconsin', 'wish', 'wit', 'withdraw', 'woman', 'women', 'won', 'wonder', 'wont', 'word', 'work', 'work hard', 'work togeth', 'worker', 'world', 'world war', 'worldwid', 'worri', 'wors', 'worst', 'worth', 'wouldn', 'wouldnt', 'wound', 'wow', 'wrap', 'write', 'writer', 'written', 'wrong', 'wrongdo', 'wrote', 'xi', 'xi jinp', 'ye', 'yeah', 'year', 'year ago', 'year later', 'year old', 'year said', 'yell', 'yemen', 'yesterday', 'york', 'york citi', 'york reuter', 'york time', 'york timeswashington', 'youll', 'young', 'young peopl', 'younger', 'youth', 'youtub', 'youv', 'zero', 'zone']\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.01)\n",
    "tfidf_vectorizer.fit(X_train_text)\n",
    "list1 = list(tfidf_vectorizer.get_feature_names())\n",
    "print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.00)\n",
    "tfidf_vectorizer.fit(X_train_text)\n",
    "list2 = list(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4536928\n",
      "['better whiter', 'hardt foreign', 'util subsidiari', 'option fraught', 'actual inning', 'friendli fierc', 'case hawk', 'thursday cole', 'winter kelli', 'injuri said', 'incom pyongyang', 'better promis', 'amend code', 'duti citizen', 'track watch', 'freez regul', ' ', 'extort demand', 'cross end', 'kushner jeff', 'mono spanish', 'rene puent', 'offend ad', 'altogeth insteadth', 'missil syria', 'uskorean', ' ', 'grip sack', 'tsipra discuss', 'outlet complet', 'clinton 2012', 'seychel england', 'treatment school', 'given special', 'beauti happen', 'tudiait volonti', 'combin capac', 'dropin follow', 'reach expect', 'split independ', 'person violat', 'fli longer', 'crisi alex', 'affirm separ', 'reduc consider', '58000 friday', 'money amaz', 'worker folk', 'truth fourth', 'pedal past', 'vividli captur', 'qin said', 'moor cite', 'entir kitchen', 'known mischief', 'come anonym', '1981 becam', 'rt surprisingli', '500 driven', 'disrupt mi', 'progressivedemocrat congression', 'temperatur brought', 'sooner lowcarbon', 'quran council', 'hint nbc', 'schiff washington', 'toxin bloodstream', 'true known', 'time choic', 'jumpsuit victim', 'erect ram', 'admir ku', 'coalit sweep', 'regret write', 'repeat thank', 'nervou got', 'observ probabl', 'prosecutor accord', 'njthi 21st', 'uphold court', 'handshak meet', 'biggest caus', 'inihelen februari', 'obama prop', 'mojo launch', 'reject tucker', 'stori think', 'atkin danteatkin', 'aim nonjudgment', 'instagram peopl', 'wast spend', 'elisefoley', 'strike planet', 'direct prefer', 'accord tmzweinstein', 'marijuana launch', 'assess presid', 'disbar misus', 'cycl ultim', 'new evid', 'defens microb', 'includ assata', 'render visual', 'captagon westernback', 'compani podesta', '2016realdonaldtrump actual', 'news outletstrump', 'flaw trump', '22 certifi', 'confidenti norwegian', 'es mentira', 'journalist schedul', 'midfebruari 2011', 'case jare', 'opaqu mnuchin', 'taken prais', 'start postdoctor', 'peruana andina', 'recal nato', 'month seriou', 'diamondandsilk januari', 'area walk', 'lose accord', 'life iamsteveharvey', 'solid product', 'christian convers', 'relentless noth', 'wing voter', 'knee solidar', 'suppos water', 'partisan postur', 'support spain', 'score state', 'zone rise', 'control burn', 'spot unit', 'return ethiopian', 'akin crimin', 'riyadhdubai reuter', 'dealt fbi', 'matter thing', 'theyr livid', 'camera instead', 'comey demand', 'contact describ', 'band knelt', 'bertelsmann foundat', 'pull line', 'enterpris sovereign', 'power margin', '', 'ngger unsavori', 'option afghanistan', 'teresa orphan', 'unpreced major', 'citizenship passport', 'swiftli recov', 'boydthes email', 'evid mismanag', 'madison wade', 'adam gingrich', 'instagram inappropri', 'architectur associ', 'crowd antifascist', 'smack seriou', 'fruit social', 'somali appar', 'piano dine', 'said bizarr', 'gambl robert', 'rive backpakistaniborn', 'exodu rohingya', 'desarrol', 'invit mani', 'opportun blaze', 'liber indic', 'went sunday', 'drama worth', 'countrifi goober', 'thought experi', 'misinform indian', 'gift limit', 'purcel declin', 'webster', 'exercis reduc', 'corpor screw', 'neighbor qatar', 'parliament legisl', 'regim anyon', 'opinion stake', 'stall implement', 'receiv militari', 'stop penn', 'flint riverwhat', 'look badg', 'year tin', 'group mere', 'locat hold', 'europ cost', 'susan chira', 'pakistan octob', 'remov oil', 'user remain', 'd', 'sleev blue', 'condemn rainbow', 'workshop east', 'park san', 'decid appeal', '60day layoff', 'shandong weiqiao', 'pay listen', 'express matter', 'rememb event', 'proceed ask', ' ', 'neg rel', 'ignor contrari', 'mood anyth', 'use crispr', 'plant shadow', 'wilmington ohiowashington', 'matter dirti', 'owner stori', 'equival constitut', 'client testifi', 'rowland aborigin', 'opportun creat', 'reform london', 'citizen client', 'read correct', 'commun import', 'surviv coupl', ' ', 'green polic', 'come comb', 'ecowa', 'launch largescal', 'le prsident', 'precaut minim', 'drop box', 'spread vast', 'need extens', 'secondari migrant', 'facebook tuesday', 'remain barnier', 'articl tv', 'regener affect', 'legal inhuman', 'worker independ', 'maher episod', 'belief secretari', 'statement remembr', 'met mother', 'got hilari', 'spark flame', 'year 000', 'grant alien', 'stalk romant', 'scientist subject', 'fact damag', 'ammunit storag', 'friend republican', 'preserv town', 'volunt sign', 'comcast usa', 'need donald', 'court close', 'artilleri offic', 'discuss conserv', 'enemi characterist', 'becam independ', 'republican castig', 'click tmsnrtrs2h0nqct', 'craft sweep', 'hope godhelpu', 'spiral footbal', 'clear februari', 'select yellen', 'elast elid', 'hillonli', 'post construct', 'violenc turf', 'basic critiqu', 'httpwwwfoxnewscompolitics2016', 'notabl neighbor', 'respons kain', 'virginia deploy', 'claim suppressor', 'telecommun comput', '84 mayor', 'claim social', 'medicar happen', 'nowinfam email', 'home hillbilli', 'francisco airport', '2016again asshol', 'help pickup', 'unclear discuss', 'gupta professor', 'lgbti australian', 'trial taiwanes', 'void republican', 'chicago philadelphia', 'dictat galact', 'fade dress', 'resid teach', 'perhap jon', 'australia end', 'transcript follow', 'russel invest', 'psammoterm', 'odessa cassiu', ' ', '2018 tax', 'east camp', 'believ talmud', 'case circuit', 'portugues brazil', '2018 continu', 'backer blatant', 'explod suicidebomb', 'joli persona', 'pufferi', 'monthli statewid', 'prosecutorsa spokesman', 'properti newcom', 'satir said', 'left fox', 'american invas', 'spicer fake', 'murphi poke', 'papadopoulo claim', 'prevent sort', 'end bronx', 'capit weekend', 'saidagain', 'virginia toyota', 'realterm valu', 'select old', 'group check', 'citizen led', 'arabia alleg', 'opportun media', 'pollut problem', 'frequent heavi', 'arsht', 'chandnani manag', 'semifin perform', 'slitfrench author', 'laugh slyli', 'choic activ', 'surpris japan', 'statement china', 'level dopamin', 'life leav', 'reviv monarchi', 'summer object', 'uncheck evolv', 'appropri help', 'updat flow', 'critic abm', 'address deal', 'worship intellig', 'nuanc tragic', 'ad perez', 'grabber ralli', 'pharmaceut result', 'chronicl blog', 'bagshot magic', 'multifacet russian', 'cite launch', 'debat leav', 'deterior deal', 'jone order', 'lift twice', 'problem oblivion', 'tech athlet', 'weak guy', 'treatment irrit', 'tonight child', 'offici registr', 'plan beirut', 'anonym hagerti', 'nuisanc substandard', 'buy children', 'vicenza chamber', 'work poorest', 'higherfe product', 'journal addit', 'feed exhaust', 'moral oppon', 'rockland', 'lie diseas', 'compromis coalit', 'apolog gay', 'eoc oral', 'submarin hide', 'louisiana beginningso', '2018 man', 'nada entsprechend', 'prais progress', 'velayati foreign', 'flag hard', 'oldthi accord', 'action rival', 'demograph break', 'given possibl', 'gravita clear', 'occupi floor', 'bogeyman toothfairi', '20yearold parakeet', 'sourc masri', 'parenthood state', 'upcom season', 'strawberri birthday', '29th', 'countri expel', 'erdoan accus', 'code cd', 'lieuten timothi', 'lowtax redtapecut', 'absgt 1415', 'way ramadi', 'reelect saw', 'taliban consid', 'asia rewrit', 'document time', 'markronald reagan', 'surprisingli fan', 'pathet said', 'democrat penc', 'tell ahead', 'tabar', 'administr resettl', 'hollywoodactingmedia', 'hope ambit', 'narrativeon', 'recogn irish', 'reserv surmis', 'seriou begin', 'beam messag', 'instanc predat', 'weaponmeanwhil', 'russia true', 'olesinski said', 'alabama schedul', 'collus zionist', 'liber explain', 'countri hemp', 'obviou legal', 'watch reput', 'punishedpregn', 'gv want', 'solidar need', 'oven tab', 'hillari grandmoth', 'today introduct', 'lead indictmentthough', 'wecatscorp', 'kudo sport', '100 site', 'come fifa', 'backlash disapprov', 'morel longtim', 'tuitionfre work', 'mar far', 'work subscrib', 'grimm warn', 'administr rid', 'sum prime', 'tenur home', 'iran aggress', 'izmir weapon', 'li remark', ' ', 'point inspir', 'eloqu said', 'aspiren', 'women stood', 'fought govern', 'prooffeatur imag', 'reflect endors', 'squar headlin', 'wisconsin tri', 'americay ll', 'cold humid', 'likewis scandalladen', 'got dnc', 'femal testosteron', 'nydaili', 'bed steve', 'prior upcom', 'student 100', 'quartz better', 'follow compon', 'everybodi bottl', 'visit charter', 'recogn underserv', 'comput shield', 'ivana comment', 'largest owner', 'want trust', 'flexibl manag', 'razor dispos', 'meet salt', 'ozon close', 'expenditur dnc', 'transport en', 'truli crime', 'oversea respond', 'mountain stage', '32 46', 'hashim nzinga', 'park mission', 'tank legal', 'person ceremoni', 'bar chocol', 'cleans program', 'invad british', 'list draw', 'vandaag', 'hand parol', 'reflect privat', 'director cair', 'time telllisten', 'guess destabil', 'open year', 'friendli comment', 'evil vote', 'cycl outsid', 'achiev mpaa', 'andor possess', 'gop overhaul', 'pressur demand', 'chicago trumpprotest', 'faith christ', 'longu red', 'steadi feet', 'cope absenc', 'iter beneficiari', 'illeg certainli', 'logic deploy', 'scandal depriv', 'dog warner', 'brandei tell', 'delito producto', 'lawmak staff', 'capit built', 'central nairobi', 'kloiber nonetheless', 'student taunt', 'onlin strategi', 'audienc eschew', 'present britain', 'industri hemp', 'write note', 'parti wauquiez', 'inde son', 'drum gop', 'pauper life', 'chung later', 'alassad besieg', 'adnan 30', 'present speaker', 'vehicl becam', 'muahab', 'selfinflict meltdown', 'want itsecret', 'head smile', 'support leftupd', 'ensur republican', 'harass tri', 'cuban televis', 'obama smart', 'editor judi', 'famili countrysid', 'standard urg', 'isi sunni', ' anwar', 'lesserknown challeng', 'hard pavement', 'explan spokeswoman', 'snapshot look', 'jay make', 'collegetodd', 'atomiqu tatsunienn', 'blm milit', 'new cost', 'guterr wrote', 'hummu cucumb', 'empathi black', 'ac sharpli', 'graciou peopl', 'banqu saudi', 'flag shop', 'later reason', 'list 2005', 'nuremberg declar', '250000 fine', 'hillari emails', 'daili china', 'unimpos arm', 'finger pontoon', 'let husband', 'novemb defiantli', 'se te', 'icahn year', 'warrant inspect', 'flush new', 'abram republican', 'day lebanon', 'buy half', 'dimpl ajmera', 'thrill new', 'jerusalem jesu', 'best katzenberg', 'class morocco', 'periodista cada', 'bonifield honestli', 'fold long', 'firm ocean', 'el espectculo', 'experi region', 'jobcreat project', 'oceanograph research', 'billion refund', 'rachel zavaleta', 'termin build', 'told soldier', 'secur theyr', 'cbtobon', 'lot upbeat', 'provinc chamchamm', 'thursday suit', 'phone heller', 'serafinowicz', 'workforc abe', 'penc announc', 'said nieto', 'mr vogel', 'republicanfor', 'dramat introduct', 'abe canada', 'true flash', 'barletta dian', 'remarksanoth woman', 'cut nois', 'leav loan', 'direct hous', 'admir comment', 'solar panel', '', 'come given', 'ban won', 'antiship cruis', 'advertisingin', 'come truespearhead', 'parti joan', 'tournament walker', 'joint instrument', 'blend moder', 'unexamin faith', 'america wkbw', 'crucial mani', 'anoth 5090', 'rehnquist appoint', 'parti kemal', '125th anniversari', 'mix bolton', 'genuin indiscrimin', 'member senategood', 'exclud like', 'mani littl', ' ', 'rival past', 'ask youll', 'dea corrupt', 'usestablish', 'albu severu', 'familiar vladimir', 'statement fenc', 'spectrum come', 'bulli ms', 'diplomat hour', 'video huckabe', 'unabl count', 'agreement insist', 'natur faith', 'humor grace', 'bombshel judici', 'coordin cynthia', 'dire timesread', 'debunk today', 'burn kizzi', 'norway spain', 'agenc memo', 'entertain isreal', 'moham seventh', 'trump disgustingli', 'precipit earli', 'forc cwru', 'ad infect', 'cri loser', 'reuter reportsin', 'record 30', 'thing student', ' ', 'root rip', 'frustrat room', 'possibl drive', 'cours describ', 'respond ms', 'hous authoritarian', 'year reveal', 'evid associ', 'ampk', 'properti argu', 'black conscious', 'washington mark', 'helmet emerg', 'throng republican', 'mainli live', 'seri benignli', '2014the burn', 'gun say', 'invis ink', 'war law', 'black magic', 'reflect solemn', 'taskbut sunday', 'path worship', 'newsmani believ', 'advisor condit', 'fail coerc', 'hot graphic', 'mourn himthat', 'policyson', 'sweeni enjoy', 'number fake', 'monument final', 'sophia', 'hous august', 'minist rare', 'alltoo power', 'specifi attempt', ' ', '2017onc republican', 'partnership systemat', 'todeschini', 'import cultiv', 'compass think', 'unrest israel', 'bank natwest', 'bhatt harvard', 'ad sicken', 'supersoldi kid', 'high dropout', 'album came', 'letter boston', 'seashor', 'championship coppel', 'event fbi', 'verma took', 'lyric deep', 'given blain', 'option approv', 'thought gon', 'clean stabl', 'condit endors', 'poland conserv', 'sea paper', 'tell thompson', 'en paralll', 'elect smith', 'deafen trump', 'financ difficult', 'ring accord', 'day destruct', 'small letter', 'quickli step', 'regain word', 'explain environ', 'intend emphas', 'overnightin', 'requir georgia', 'world crazi', 'truth prevail', 'cum ad', 'clock remov', 'week near', 'start dormanc', 'foreign partner', 'fethullah gulen', 'want factor', 'cleans situat', 'govern defeat', 'mostli gay', 'nonprofit financi', 'wolv helicopt', 'theater stranger', 'problem america', 'assist chief', 'opportunist look', 'say facad', 'pose inher', 'websit abil', 'low 20', 'hard soul', 'myerson', 'origin fuel', 'law magnitski', 'nowmonday', 'matti omaha', 'swastika egyptian', 'tinderdri unusu', 'possibl savoir', 'southeast brazil', 'nehawu cape', 'billionair concern', 'martha tell', 'pickup jam', 'programa que', 'perhap lot', 'book deserv', 'livestream site', 'diploma languag', 'comparison went', 'thorpviol', 'countri sooner', 'saturday address', 'trip breath', 'god folk', 'personnel outsid', 'food chicago', 'hard trace', 'litani crime', 'tagebuch', 'meanwhil benefit', '23102016', 'day studi', 'includ appointe', 'danielss idiosyncrat', 'prepar away', 'takeov phylli', 'myanmar accept', 'struggl nonetheless', 'societi illequip', 'strang fate', 'desir woman', 'shakeer', 'execut kentucki', 'run swim', 'tannehil knee', 'ministri misanthrop', '15 partsperbillion', 'parent trap', 'republican redeem', 'decemb wada', 'treatment acupunctur', 'end nikisch', 'rio new', 'mile 285', 'transfer iran', 'vicki gray', 'wall subsequ', 'fail aircraft', 'new practic', 'journey wilsonmil', 'similar defens', 'nfa', 'behalf inconceiv', 'place aston', 'share america', 'vote industri', 'agricultur reimburs', 'tonight boiler', ' ', 'rival stay', 'hancock johnson', 'gosk brand', 'nucleararm china', '41000 1980', 'chairman safeti', 'memo ap', 'statist denmark', 'balanc 4000', 'bohn cofound', 'univers known', 'biggest north', 'malik featur', 'strong uk', 'iran undeliv', 'nurs tendenc', 'marri ruler', 'appear chicago', 'demonstr punch', 'walk far', 'lie ultim', 'highest televis', 'tunisian born', 'execut promis', 'bannon 2015', 'shirt brown', 'brace shatter', 'strong mind', 'continu revolt', 'marshal bank', 'tag post', 'women prime', 'ruptur friendship', 'premaidan video', 'pediatr condit', 'detaine gitmo', 'got unrealit', 'invest 500', 'consensu fec', 'treatment internet', 'merchandis outpost', 'green climat', 'vengeanc clinton', 'pilot saw', 'fink blackrock', 'currenc immediaci', 'act 1991', 'photo moment', 'fiebr lo', 'twominut fit', 'visual artist', 'watch develop', 'ask honolulu', 'powel reveal', 'villag watch', 'individu brake', 'hud sec', 'provoc rhetor', 'remov 93yearold', 'dr metspalu', 'democrat palin', 'mrgregshield', 'credit 2000', 'themorganleigh', 'daysrealdonaldtrump receiv', 'hall ministri', 'natur left', 'long manson', 'park 320', 'year breitbartth', 'control socialist', 'overrid requir', 'anguish anger', 'anoth senseless', 'matter pull', 'neck himher', 'smaller seven', 'casebook inspector', 'scout mani', 'resid emin', 'administr particip', 'encofrador', 'puppet roger', 'aspect 2016', 'gener troubl', 'link head', 'muchneed bridg', 'referendum booth', '15 2017she', 'alreadi recogn', 'goe goe', 'list julia', 'engin mani', 'end namesak', 'meredith corp', 'trump sheet', 'daughter west', '10th yer', 'germani recognit', 'seen 3000', 'wastberg swedish', 'special ballist', 'foreign vet', 'ignor warn', 'jersey suburb', 'previous set', 'engin justifi', 'newsfe media', 'want plow', 'freer said', 'open peacekeep', 'peopl stare', 'vengeanc anoth', 'dredg old', 'reason reid', 'throw jeopardi', 'clearli awar', 'realist moment', 'simpli unconstitutionalthi', 'host millionair', 'test clearli', 'true reason']\n"
     ]
    }
   ],
   "source": [
    "unique = set(list2) - set(list1)\n",
    "print(len(list(unique)))\n",
    "print(list(unique)[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n",
      "['2016', 'accord', 'act', 'ad', 'administr', 'allow', 'america', 'american', 'anoth', 'appear', 'ask', 'attack', 'becom', 'believ', 'campaign', 'case', 'chang', 'citi', 'claim', 'clinton', 'close', 'come', 'comment', 'continu', 'countri', 'critic', 'day', 'democrat', 'donald', 'donald trump', 'elect', 'end', 'everi', 'face', 'fact', 'feder', 'follow', 'forc', 'gener', 'good', 'govern', 'group', 'happen', 'help', 'hillari', 'hillari clinton', 'hous', 'imag', 'includ', 'issu', 'know', 'law', 'lead', 'leader', 'live', 'long', 'look', 'major', 'make', 'mani', 'mean', 'media', 'meet', 'member', 'million', 'month', 'nation', 'need', 'new', 'new york', 'news', 'number', 'obama', 'offic', 'offici', 'order', 'parti', 'peopl', 'person', 'place', 'plan', 'point', 'polici', 'polit', 'possibl', 'post', 'power', 'presid', 'presidenti', 'public', 'question', 'realli', 'recent', 'report', 'repres', 'republican', 'respons', 'reuter', 'right', 'run', 'said', 'say', 'secur', 'senat', 'sever', 'start', 'state', 'statement', 'support', 'talk', 'thing', 'think', 'time', 'told', 'took', 'tri', 'trump', 'tuesday', 'turn', 'unit', 'unit state', 'use', 'video', 'vote', 'want', 'washington', 'way', 'week', 'white', 'white hous', 'work', 'world', 'year', 'york']\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.15)\n",
    "tfidf_vectorizer.fit(X_train_text)\n",
    "list3 = list(tfidf_vectorizer.get_feature_names())\n",
    "print(len(list3))\n",
    "print(list3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection for Additional Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Added Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = pd.read_csv(\"Final datasets/train_data.csv\")\n",
    "# val_data_features = pd.read_csv(\"Final datasets/val_data.csv\")\n",
    "test_data_features = pd.read_csv(\"Final datasets/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data_features[\"class_label\"].values\n",
    "y_test = test_data_features[\"class_label\"].values\n",
    "# y_val = val_data_features[\"class_label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All added features for min_df = 0.01 (3k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise TfidfVectorizer with min_df = 0.01 as per feature selection\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1,2), min_df=0.01)\n",
    "\n",
    "# Create mapper object to combine added features and tfidf word vectors\n",
    "mapper = DataFrameMapper([\n",
    "    (['char_count', 'word_count', 'sentence_count', 'prop_unique_words',\n",
    "    'avg_sentence_length', 'prop_punctuations', 'prop_stopwords',\n",
    "    'prop_words_in_quotes', 'prop_nouns', 'prop_verbs', 'prop_adjectives',\n",
    "    'prop_discourse_relations', 'textblob_sentiment'], None), \n",
    "    ('text_preprocessed', tfidf_vectorizer)\n",
    "])\n",
    "\n",
    "# fit_transform mapper on train data with added features and transform test data with added features\n",
    "X_train_added_features = mapper.fit_transform(train_data_features)\n",
    "X_test_added_features = mapper.transform(test_data_features)\n",
    "# X_val_added_features = mapper.transform(val_data_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define logistic regression model\n",
    "log_reg_clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      6361\n",
      "           1       0.96      0.94      0.95      6660\n",
      "\n",
      "    accuracy                           0.95     13021\n",
      "   macro avg       0.95      0.95      0.95     13021\n",
      "weighted avg       0.95      0.95      0.95     13021\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "log_reg_clf.fit(X_train_added_features, y_train)\n",
    "y_pred = log_reg_clf.predict(X_test_added_features)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected added features for min_df = 0.01 (3k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      6361\n",
      "           1       0.97      0.93      0.95      6660\n",
      "\n",
      "    accuracy                           0.95     13021\n",
      "   macro avg       0.95      0.95      0.95     13021\n",
      "weighted avg       0.95      0.95      0.95     13021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapper = DataFrameMapper([\n",
    "    (['char_count', 'word_count', 'prop_unique_words', 'avg_sentence_length', 'prop_punctuations', 'prop_stopwords', 'prop_nouns'], None), \n",
    "    ('text_preprocessed', tfidf_vectorizer)\n",
    "])\n",
    "\n",
    "# fit_transform mapper on train data with added features and transform test data with added features\n",
    "X_train_added_features = mapper.fit_transform(train_data_features)\n",
    "X_test_added_features = mapper.transform(test_data_features)\n",
    "\n",
    "log_reg_clf.fit(X_train_added_features, y_train)\n",
    "y_pred = log_reg_clf.predict(X_test_added_features)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All added features for min_df = 0.15 (134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.91      6361\n",
      "           1       0.92      0.91      0.91      6660\n",
      "\n",
      "    accuracy                           0.91     13021\n",
      "   macro avg       0.91      0.91      0.91     13021\n",
      "weighted avg       0.91      0.91      0.91     13021\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Initialise TfidfVectorizer with min_df = 0.01 as per feature selection\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1,2), min_df=0.15)\n",
    "\n",
    "# Create mapper object to combine added features and tfidf word vectors\n",
    "mapper = DataFrameMapper([\n",
    "    (['char_count', 'word_count', 'sentence_count', 'prop_unique_words',\n",
    "    'avg_sentence_length', 'prop_punctuations', 'prop_stopwords',\n",
    "    'prop_words_in_quotes', 'prop_nouns', 'prop_verbs', 'prop_adjectives',\n",
    "    'prop_discourse_relations', 'textblob_sentiment'], None), \n",
    "    ('text_preprocessed', tfidf_vectorizer)\n",
    "])\n",
    "\n",
    "# fit_transform mapper on train data with added features and transform test data with added features\n",
    "X_train_added_features = mapper.fit_transform(train_data_features)\n",
    "X_test_added_features = mapper.transform(test_data_features)\n",
    "\n",
    "log_reg_clf.fit(X_train_added_features, y_train)\n",
    "y_pred = log_reg_clf.predict(X_test_added_features)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected added features for min_df = 0.15 (134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90      6361\n",
      "           1       0.94      0.86      0.90      6660\n",
      "\n",
      "    accuracy                           0.90     13021\n",
      "   macro avg       0.90      0.90      0.90     13021\n",
      "weighted avg       0.90      0.90      0.90     13021\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "mapper = DataFrameMapper([\n",
    "    (['char_count', 'word_count', 'prop_unique_words', 'avg_sentence_length', 'prop_punctuations', 'prop_stopwords', 'prop_nouns'], None), \n",
    "    ('text_preprocessed', tfidf_vectorizer)\n",
    "])\n",
    "\n",
    "# fit_transform mapper on train data with added features and transform test data with added features\n",
    "X_train_added_features = mapper.fit_transform(train_data_features)\n",
    "X_test_added_features = mapper.transform(test_data_features)\n",
    "\n",
    "log_reg_clf.fit(X_train_added_features, y_train)\n",
    "y_pred = log_reg_clf.predict(X_test_added_features)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "729bae39908f34aae3ae4cd67793570f1853d7623135d20860991de8be7ba981"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
