{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# Packages for NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "\n",
    "# Packages for sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Packages for visualisation \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Packages for data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# Packages for machine learning modelling\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "# precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Packages for sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Packages for visualisation \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Packages for MLP\n",
    "import nltk\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../Data/Combined data/train_data.csv\", index_col=0)\n",
    "val_data = pd.read_csv(\"../Data/Combined data/val_data.csv\", index_col=0)\n",
    "test_data = pd.read_csv(\"../Data/Combined data/test_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating functions to get various features\n",
    "\n",
    "def get_pos_tags(text): # POS tags reference: https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/ \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    return pos_tags\n",
    "\n",
    "def get_num_nouns(text):\n",
    "    nouns_list = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    pos_tags = get_pos_tags(text)\n",
    "    nouns_count = len([word for (word, pos) in pos_tags if pos in nouns_list])\n",
    "    return nouns_count\n",
    "\n",
    "def get_num_verbs(text):\n",
    "    verbs_list = ['VB', 'VBD', 'VBG', 'VBN', 'VDP', 'VBZ']\n",
    "    pos_tags = get_pos_tags(text)\n",
    "    verbs_count = len([word for (word, pos) in pos_tags if pos in verbs_list])\n",
    "    return verbs_count\n",
    "\n",
    "def get_num_adj(text):\n",
    "    adj_list = ['JJ', 'JJR', 'JJS']\n",
    "    pos_tags = get_pos_tags(text)\n",
    "    adj_count = len([word for (word, pos) in pos_tags if pos in adj_list])\n",
    "    return adj_count\n",
    "\n",
    "def get_num_discourse(text):\n",
    "    discourse_keywords = ['even then', 'as though', 'still', 'whereas', 'on the other hand', 'but', 'while', 'ultimately', 'if', 'even when', 'instead', 'next', 'when', 'on the one hand indeed', 'even still', 'in the end', 'meanwhile', 'separately', 'or', 'nonetheless', 'neither', 'in contrast', 'nevertheless', 'although', 'then', 'in turn', 'regardless', 'as much as', 'rather', 'meantime', 'much as', 'yet', 'however', 'even as', 'conversely', 'even after', 'nor', 'finally', 'as if', 'in fact', 'also', 'even if', 'by comparison', 'and', 'besides', 'by contrast', 'on the contrary', 'even though', 'though']\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    discourse_count = len([word for word in tokens if word in discourse_keywords])\n",
    "    return discourse_count\n",
    "\n",
    "def get_num_stopwords(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stopword_count = len([word for word in tokens if word in stopwords.words('english')])\n",
    "    return stopword_count\n",
    "\n",
    "def get_num_punctuations(text):\n",
    "    punctuations = '!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~'\n",
    "    punctuation_count = len([char for char in text if char in punctuations])\n",
    "    return punctuation_count\n",
    "\n",
    "def get_num_words_in_quotes(text):\n",
    "    quotes = re.findall(\"'.'|\\\".\\\"\", text)\n",
    "    quote_count = 0\n",
    "    if quotes is None:\n",
    "        return 0\n",
    "    else:\n",
    "        for quote in quotes:\n",
    "            words_in_quote = quote[1:-1]\n",
    "            quote_count += len(words_in_quote.split())\n",
    "        return quote_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(dataframe):\n",
    "    \"\"\"\n",
    "        Adds 13 additional features to an input dataframe and returns the updated dataframe\n",
    "    \"\"\"\n",
    "    num_nouns = []\n",
    "    num_verbs = []\n",
    "    num_adj = []\n",
    "    num_discourse = []\n",
    "    num_stopwords = []\n",
    "    num_punctuations = []\n",
    "    num_quote_words = []\n",
    "\n",
    "    for row in dataframe['text']:\n",
    "        nouns_count = get_num_nouns(row)\n",
    "        num_nouns.append(nouns_count)\n",
    "\n",
    "        verbs_count = get_num_verbs(row)\n",
    "        num_verbs.append(verbs_count)\n",
    "        \n",
    "        adj_count = get_num_adj(row)\n",
    "        num_adj.append(adj_count)\n",
    "        \n",
    "        discourse_count = get_num_discourse(row)\n",
    "        num_discourse.append(discourse_count)\n",
    "\n",
    "        stopword_count = get_num_stopwords(row)\n",
    "        num_stopwords.append(stopword_count)\n",
    "\n",
    "        punctuation_count = get_num_punctuations(row)\n",
    "        num_punctuations.append(punctuation_count)\n",
    "\n",
    "        quote_count = get_num_words_in_quotes(row)\n",
    "        num_quote_words.append(quote_count)\n",
    "    \n",
    "    dataframe['char_count'] = dataframe['text'].apply(lambda x: len(str(x)))\n",
    "    dataframe['word_count'] = dataframe['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "    dataframe['sentence_count'] = dataframe['text'].apply(lambda x: len(str(x).split(\".\")))\n",
    "    dataframe[\"num_unique_words\"] = dataframe['text'].apply(lambda x: len(set(str(x).split(\" \"))))\n",
    "    dataframe[\"avg_sentence_length\"] = dataframe['word_count']/dataframe['sentence_count']\n",
    "    dataframe['num_punctuations'] = num_punctuations\n",
    "    dataframe['num_stopwords'] = num_stopwords\n",
    "    dataframe['num_words_in_quotes'] = num_quote_words\n",
    "    dataframe['num_nouns'] = num_nouns\n",
    "    dataframe['num_verbs'] = num_verbs\n",
    "    dataframe['num_adjectives'] = num_adj\n",
    "    dataframe['num_discourse_relations'] = num_discourse\n",
    "    dataframe['textblob_sentiment'] = dataframe['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running create_features and output a new csv\n",
    "\n",
    "train_data_added_features = create_features(train_data)\n",
    "val_data_added_features = create_features(val_data)\n",
    "test_data_added_features = create_features(test_data)\n",
    "\n",
    "train_data_added_features.to_csv(\"../Data/Data with added features/train_data_added_features.csv\")\n",
    "val_data_added_features.to_csv(\"../Data/Data with added features/val_data_added_features.csv\")\n",
    "test_data_added_features.to_csv(\"../Data/Data with added features/test_data_added_features.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising added features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD BOXPLOTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing base model performance with/without added features using TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating X_train_added_features and X_test_added_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise TfidfVectorizer with min_df = 0.01 as per feature selection\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1,2), min_df=0.01)\n",
    "\n",
    "# Create mapper object to combine added features and tfidf word vectors\n",
    "mapper = DataFrameMapper([\n",
    "    (['char_count', 'word_count', 'sentence_count', 'num_unique_words', 'avg_sentence_length', 'num_punctuations', 'num_stopwords', 'num_words_in_quotes', 'num_nouns', 'num_verbs', 'num_adjectives', 'num_discourse_relations', 'textblob_sentiment'], None), \n",
    "    ('text_preprocessed', tfidf_vectorizer)\n",
    "])\n",
    "\n",
    "# fit_transform mapper on train data with added features and transform test data with added features\n",
    "X_train_added_features = mapper.fit_transform(train_data_added_features)\n",
    "X_test_added_features = mapper.transform(test_data_added_features)\n",
    "print(X_train_added_features.shape)\n",
    "print(X_test_added_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating X_train, X_test and y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf_vectorizer.fit_transform(train_data[\"text_preprocessed\"].values)\n",
    "X_test = tfidf_vectorizer.transform(test_data[\"text_preprocessed\"].values)\n",
    "\n",
    "y_train = train_data[\"class_label\"].values\n",
    "y_test = test_data[\"class_label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing base model - Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base model\n",
    "naive_bayes_clf = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_clf.fit(X_train, y_train)\n",
    "y_pred = naive_bayes_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing base model - Added Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_clf.fit(X_train_added_features, y_train)\n",
    "y_pred = naive_bayes_clf.predict(X_test_added_features)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing base model - Added Features (Scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale X\n",
    "X_train_added_features_scaled = MinMaxScaler().fit_transform(X_train_added_features)\n",
    "X_test_added_features_scaled = MinMaxScaler().fit_transform(X_test_added_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_clf.fit(X_train_added_features_scaled, y_train)\n",
    "y_pred = naive_bayes_clf.predict(X_test_added_features_scaled)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "729bae39908f34aae3ae4cd67793570f1853d7623135d20860991de8be7ba981"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
